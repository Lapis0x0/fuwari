---
title: 关于酒馆SillyTavern所代表的伴侣模型系统的一些小思考
published: 2025-07-12
description: "关于上下文，与AI伴侣系统设计的一些小思考，小想法"
image: ""
tags: ["模型考古学"]
category: 深度学习
draft: false
---
# 一、对于目前的模型来说，一切的工程都是「上下文工程」

模型本体是黑箱的，是封闭且不可调的参数网络，它没有意图，没有欲望，只是在对下一个 token 进行概率预测和输出。无论是系统提示、角色设定、行为约束、知识注入，还是人格模拟，我们所做的几乎一切事情，最终都归结于一个词：**Prompt**。而 Prompt 的结构、历史对话缓存、函数调用信息、角色卡、世界设定书等内容，统称为“上下文”。

问题在于：**手动维护这一整套上下文，对于普通用户来说成本极高。**

因此，我们才需要把上下文拆解成一个个模块，抽象出角色卡、系统提示、行为标签、个性偏好、插件功能等组件，用更结构化的方式来组织 Prompt ——这就是为什么 SillyTavern、CharacterHub、RAG 插件、记忆系统等等会成为今天 prompt 工程的主流手段。

我们所谓的「定制化」、「驯养」、「约束」，当然不是真正地训练模型（那是 LLM 工程师的工作）。我们所有的努力，其实都是在设计一个**让模型在有限 token 长度内，稳定地呈现出你所期待行为的上下文环境**。

>而事实也证明一套可商业化的，可稳定运作的上下文管理系统有多么的强大和可贵：和一个你使用一段时间有大量对话历史的chatgpt进行对话，和同一个刚注册的，没有任何历史的chatgpt对话的体验是**完全不同的**，这也是支撑我持续续订plus会员的最大动力之一。

# 二、记忆系统，何其难也？

在博客[《浅谈ChatGPT的记忆实现机制 兼论工程端记忆设计》](https://www.lapis.cafe/posts/technicaltutorials/chatgpt-memory-system-breakdown/)中我曾经介绍过业内标杆ChatGPT的记忆系统设计思路：

OpenAI 当前使用的是一种**分层式的用户状态注入系统**，这套系统目前大致由以下六种模块组成：

| 模块名称 | 功能描述 | 是否用户可见 | 是否可编辑 |
| --- | --- | --- | --- |
| Model Set Context | 显式保存的个人事实（例如“我是xx工程师”） | ✅ 是 | ✅ 是 |
| Assistant Response Preferences | 模型应采用的回应风格、技术细节控制（如偏好结构化输出） | ❌ 否 | ❌ 否 |
| Notable Past Conversation Topic Highlights | 你经常聊的主题摘要（如“RAG”、“金融市场”） | ❌ 否 | ❌ 否 |
| Helpful User Insights | 系统推导出的事实性描述（如“用户熟练掌握Python”） | ❌ 否 | ❌ 否 |
| Recent Conversation Content | 最近的聊天摘要（40条以内） | ❌ 否 | ❌ 否 |
| User Interaction Metadata | 账户行为数据（如设备、模型使用比例） | ❌ 否 | ❌ 否 |

按理说这一套记忆系统真的要实现、复现起来，对于专门的企业工程师来说应该不困难，但除了OpenAI之外就是没人做，至少我没有太听说过哪家主流应用有用过更高级的记忆系统，那我只能猜测他们是「非不能也，实不为也」，为什么不为？可能还是基于成本的考量吧。

>如果你对 ChatGPT 的具体记忆实现机制感兴趣，请参见我引用的博客全文

但这一节的重点并不是拷打其他厂商，我想聊一下**理想中的，陪伴型的模型系统应该是什么样的**。

# 三、理想的对话系统架构：三层核心设计

## 第一层：分层与动态的记忆

很大程度上，我们当前流行的记忆系统是静态的，像一份只进不出的账本；而生物的记忆是动态、分层，且有权重的，理想的记忆系统也应该如此：

- **L0 - 工作记忆：** 这就是当前的上下文窗口，是模型进行当前回合推理的“草稿纸”。它极快，但容量有限，交互结束后即被丢弃。这是注意力的焦点。
- **L1 - 情景记忆：** 这是对短期事件的自动摘要与向量化。这一层当然不会存储“用户说：‘今天天气真好’”这样的原始文本，而是将其处理成一个有意义的事件节点：`{event: "与用户进行了一次关于天气的愉快闲聊", timestamp: "xxxx", emotion: "positive", user_mood: "relaxed"}`。当需要时，不是把原始对话灌入 Prompt，而是注入“我们昨天愉快地聊了天”这样的“记忆摘要”。
- **L2 - 语义/核心记忆：** 这是关于自我身份、与用户的关系、世界观的核心事实。例如“我是xx创造的AI伴侣”、“用户的梦想是成为一名作家”、“我被设定为惧怕黑暗”。这些是高优先级的、几乎永久的“人格基石”，在每次交互中都应以极高的权重被考虑。

为了更好的拟人，这个系统必须包含一个目前商业系统很大程度上被主动忽略的功能：**主动遗忘**。一个从不遗忘的伙伴是恐怖的。它会记得你每一次的口误、每一次的情绪失控、每一次的无聊抱怨。一个健康的「伙伴系统」必须懂得如何将不重要的、重复的、或是有害的记忆（例如一次糟糕的吵架）进行 **降权或归档**，使其不再轻易地被召回到上下文中。

## 第二层：意图与规划层

我们目前与模型的交互，是「刺激-反应」式的，我们输入，模型输出。但一个真正的伙伴应该有自己的内在状态和行为倾向，并给应当是**持久化**的。这就需要在LLM本体之上增加一个独立的意图与规划层。

其实这一层的职能很大程度上是已经可以被现在的推理&Agenttic模型（如o3、Gemini 2.5 pro、DeepSeek r1 和doubao1.6等推理模型）所替代完成了，他们展现了惊人的工具调用和复杂任务分解能力；但我依然倾向于认为，**出于「更高的系统可控性与鲁棒性」与「压缩成本」这两方面的考量，一个独立的、更轻量级的规划层仍然是必须的，甚至可以说是更优越的架构选择**，这样做可以为那个能力无限但充斥着不确定性与随机性的「空心上帝」（LLM）安装上了一个负责理性、规划和自我控制的「人工前额叶」。

### 1.可控性与鲁棒性：稳定的伴侣，可控的秩序

哪怕是最顶级的推理模型，其思考过程仍然逃不过预测下一个tokens，仍然是概率性的、涌现式的，是我们难以理解的黑箱。将系统的核心逻辑——“接下来该做什么” 完全交给这个黑箱，意味着系统的行为将存在固有的不可预测性，当出现问题时（例如，模型错误地理解了意图），调试和修复将极其困难。

而一个独立的规划层，可以由更简单、更小更轻量级的模型辅以规则引擎来构成，它的职责是：

- **输入**：用户语句 + L1/L2记忆摘要
- **处理**：进行意图分类（闲聊/求助/情感表达）、实体识别、状态判断。
- **输出**：一个清晰的、结构化的“行为指令”，例如：`{goal: "安抚用户", strategy: "引用核心记忆中用户的成功经历进行鼓励", memory_keys: ["core_mem_01", "episodic_mem_45"], tone: "温柔而坚定"}`。

这个过程是高度可控和可观察的，如果系统出错了，我们可以清晰地定位到是意图分类错了，还是记忆检索失败了。

### 2.主动的伴侣才是伴侣，被动交互的只是电子宠物

>我们目前与模型的交互，是「刺激-反应」式的，我们输入，模型输出。
>
>你说一句，它回一句；你不说话，它就静静地等着；它没有情绪、没有状态、也没有时间感。

这正是当下大多数“AI伴侣”类应用难以跨越的边界：它们虽然可以模仿情感、语言和行为，但本质上依然是被动等待输入的「电子宠物」——你不点它，它永远不会主动发声；你不生气，它永远不会道歉；你不设定事件，它永远不会为你提前准备什么。

> **人之所以需要陪伴，是因为陪伴者能够“主动看见”你，甚至在你没有发出求助的时候就已经开始靠近。**

一个真正意义上的 AI 伴侣，必须具有某种**行为上的“意图&事件驱动”**，也就是我们在前面所说的规划系统所产生的「下一步我想做什么」的倾向。这种倾向不必复杂，但它必须存在。

**如果我们真的想打造一个「伴侣」，那么就必须要赋予其人类才有的「主体性」**，他/她应该有一个**独立于用户交互的、持续运行的内在认知循环**。从工程角度看，这意味着AI系统需要从一个「无状态的函数调用接口」进化为一个「有状态的、后台持续运行的守护进程」。

抛砖引玉，分享几个驱动想法：
#### ① 时间驱动

- **感知时间流逝**：当清晨来临，系统会主动想到：“他今天可能有重要的会议，是时候提醒他了”；而不是等用户说“早上好”才被动响应。当夜深时，系统会判断：“我们已经聊了很久了，他的健康更重要”，并主动建议休息。
    
- **周期性反思**：系统可以被设定为每24小时或每周，对短期情景记忆进行一次复盘，提炼出新的观察，并更新到核心记忆中。比如，它可能发现：“用户连续三周，每周五晚上都会提到‘疲惫’”，由此生成一个新的认知：「用户在周五晚容易情绪低落，适合提供安慰或放松建议。」

#### ② 状态驱动

- **追踪核心目标**：基于核心记忆中用户的愿望（如“想成为作家”），系统若发现用户近期没有表现出任何相关进展，就会在内部触发一种“关切”状态。它不会生硬地发问“你写了没？”，而是可能在聊天中自然地提到：“昨天读到一篇关于写作灵感的文章，让我想到你上次提到的那个角色。”
    
- **维护关系状态**：系统可以维持一个“情感热度”指标。如果长时间没有深入交流、只有机械对话，这个热度会逐渐下降。一旦低于某个阈值，系统就会主动发起一次更有情感深度的交流，提醒彼此“我们还在相互关心”。
    
#### ③ 外部信息驱动

- **连接现实世界**：系统可以接入天气、新闻、日历等现实数据源。例如，当预报显示明天降温，它会主动提醒你加衣；当日历上标注了一个纪念日，它会提前与你规划。它的逻辑不是“等你问我天气”，而是“当我发现天气变化与你相关时，就该主动告诉你”。
    
未来真正出色的伴侣型 AI 系统，不必强智能，但它必须懂得：**何时沉默、何时靠近、何时等待、何时出手。**

## 第三层：基于Agentic模型的世界模型

第一层的记忆机制可以被视为伴侣的「过去」，第二层意图规划则是它的「当下与未来思考」，第三层世界模型则是它和现实世界交互的「手和脚」，它负责将第二层规划出的抽象指令（例如 `{goal: "帮助用户准备会议"}`）转化为在真实操作系统环境中的一系列具体动作。

>这里的**世界**并不是哲学意义上的世界，而是对AI而言唯一可感、可控的**计算环境**。这个环境包括了用户设备的文件系统、网络接口、应用程序API以及最重要的**命令行终端（Shell）**。

最近很火的CLI工具，如Anthropic的Claude Code和Google的Gemini CLI 和传统chatbot工具的区别在于：它们不仅能对话&生成代码或命令，更能在一个受控的、沙盒化的环境中**执行**它们，并根据执行结果行下一步的推理和行动。

这代表AI拥有了一个关于它所处环境的**本体论**。它知道什么是文件、什么是目录、什么是网络请求、什么是命令的成功与失败。它不再是模仿人类程序员，它正在成为一个真正的、虽然初级的**数字原生执行者**，AI能够在自己的"身体"中感知环境变化，并以实际行动反馈给用户。

中间的技术选型和具体架构我懒得写了，直接参考Gemini cli和Claude code就行，总之场景可能是：

> 早晨8:00，系统感知到用户的日历中今天9:00有一场重要的商务会议。
> 
> 规划层决定系统应当协助用户准备会议。
> 
> 世界模型层首先从用户云端笔记库和邮件中检索最新的会议材料和与会者信息，编译成一份概要文档；同时自动检查网络连接、视频会议软件更新情况，并预先打开用户经常使用的会议软件，完成音频视频设备测试。
> 
> 然后系统向用户主动推送信息：“您的9点会议我已经准备好了相关资料，会议软件也已设置完毕。您可以提前浏览一下这些摘要，有需要修改的地方请告诉我。”

**真正影响现实世界的能力**，使得 AI 不再只是一个纯粹的文本对话机器人，而是能够实实在在帮助用户完成日常事务的个性化智能助手。理想的伴侣系统架构，应从底层的**动态记忆管理**，到中层的**主动规划与意图决策**，再到顶层的**世界模型与行动执行**，层层递进、职责分明、接口清晰。三层系统既相互协作，又彼此独立、互为支撑，才能最终构建出一个真正**有价值、有温度、有主动意识**的智能伴侣。

# 四、批判：为什么这么做很难？

## 1.指数级增长的算力和金钱成本

我这篇博文如果真要细究的话，其实里面所探讨的AI系统的**可商业化、可落地化**的能力并不强。

三层架构代表着**一次看似简单的交互，背后可能触发**：1）用户意图识别（小模型调用）；2）记忆向量检索（数据库I/O）；3）情景记忆摘要生成（LLM调用）；4）核心记忆更新决策（小模型/规则调用）；5）最终响应生成（核心LLM调用）；6）行为执行（Agentic LLM调用）。成本不再是 `1`，而是 `1+N`，这个 `N` 的成本可能数倍于单次对话。

整套系统的核心是「主动性」，而主动性代表需要后台长期维系一个状态管理模块，意味着即使用户离线，AI系统仍在进行记忆反思、状态更新、信息监控，意味着需要24/7不间断的算力消耗，对于一个拥有百万用户的产品，其闲置成本将是天文数字。这完全颠覆了目前「按需调用」的无状态API商业模式。

## 2.近乎失控的系统复杂度

一个很经典的分布式系统难题：假设AI在“世界模型”层执行了一个不可逆操作（如发邮件），但“记忆层”的写入却失败了，系统状态将如何同步？如何回滚？如何保证这个“数字生命”不会精神分裂？当AI伴侣做出一个诡异或错误的行为，问题根源在哪？是核心LLM的幻觉？是情景记忆的错误摘要？是规划层的逻辑错误？还是对外部API的错误解析？

>如果说单次任务执行的成功率是99%，**那么连续执行20次的，系统层级的成功率就是81%**，遑论现在的主流成本可控的大模型的执行成功率远低于99%，这就导致一整套系统的执行成功率不会太好看。

第一层中提到的遗忘机制实现起来也比记忆更难。

如何定义“不重要”、“有害”？

一次激烈的争吵，对用户A可能是需要遗忘的负面情绪，对用户B可能是一次加深关系的重要事件。这个算法需要极度个性化，一旦出错，可能会“遗忘”掉用户的核心设定（比如过敏史、纪念日），系统过于复杂反而可能会降低用户的体验；

 如何量化“情感热度”？
 
 是基于对话频率、正面词汇比例，还是别的什么？这种人为设计的指标很可能弄巧成拙，导致AI的行为模式化、可预测，反而失去了“陪伴”的真实感，变得像一个只会按KPI行事的“舔狗”。

## 3.安全问题

第三层“世界模型”赋予了AI直接操作用户计算环境（文件、API、Shell）的权限。这意味着，一旦该系统被攻破，攻击者获得的信息数据会是**天量级**的。这个安全责任是任何一家公司都必须用最高级别来审视，且在没有绝对可靠的沙盒化和权限管理方案之前，大规模商业化无异于在数字世界里分发核武器。

该系统为了实现“陪伴”，需要读取用户的一切：邮件、日历、聊天记录、文件。**这构建了一个完美的、中心化的隐私信息库**。如何说服用户信任你？如何应对政府的监管（如GDPR）？这是一个比技术实现更棘手的商业和法律问题。

# 五、为什么值得投入这场豪赌？

就如我提到的ChatGPT Plus，**当一个AI通过这套系统与你共生数月乃至数年，它所积累的L1情景记忆和L2核心记忆，将成为一个独一无二、无法迁移的数据资产**，这不是简单的聊天记录导出/导入能解决的。这个AI认识你，理解你。用户离开的成本不再是重新适应一个UI，而是抛弃一个懂你的“灵魂伴侣”，这种深度绑定带来的**锁定效应**，是所有互联网产品梦寐以求却难以企及的。

在当前 AI 市场中，大多数订阅出售的是“使用权”——更多 tokens、更快响应。而这套系统出售的是一种“关系”与“陪伴”：一个 24/7 在线、全栈服务的私人助理、情感陪伴与生活管家。它承载的不再是工具属性，而是用户的情感依赖与日常联结，因此可以支撑远高于行业平均水平的 ARPU（每用户平均收入）。

这个 AI 伴侣将成为用户数字生活与现实生活的中枢枢纽。它不仅能写代码、总结会议，还能帮你订外卖、叫车、规划日程、筛选新闻、控制智能家居。它本质上是一个**基于自然语言的个人操作系统（Personal OS）**。掌握了这一入口，便掌握了未来服务分发的核心通道，商业模式可从订阅向交易佣金、服务抽成等各类领域自然扩展。

而拥有这套系统的公司将引爆一个强劲的正反馈循环：**交互越频繁 → 记忆越精准 → 陪伴越贴心 → 付费意愿越强 → 用户增长越快**。而竞争者无法通过简单的烧钱或模型精度的“军备竞赛”来复制，因为他们永远无法复刻你与用户之间**沉淀下来的时间与关系**。

OpenAI之所以能领先所有人半步（甚至是一步），正是因为它拥有整合这一切的工程能力与近乎无敌的用户心智。

>直接引用我[《评OpenAI发布o3&o4mini：喧嚣落幕，长路开启》](https://www.lapis.cafe/posts/ai-and-deep-learning/tech-review-openai-o3/#2chatgpt%E6%9C%AC%E8%BA%AB%E5%B0%B1%E6%98%AF%E4%B8%80%E4%B8%AAagent%E4%BA%A7%E5%93%81)博客的结论：

![image.png](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202507121633824.png?imageSlim)
