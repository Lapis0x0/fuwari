---
title: 模型考古学（八）：Llama4发布——并非领先
published: 2025-04-06
description: "Meta发布Llama 4系列，采用MoE、原生多模态与千万级上下文。虽性能宣称领先，但社区质疑实际效果与宣传不符，发布显仓促，更像追赶而非引领。"
image: "https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202504061504868.png"
tags: ["模型考古学"]
category: 深度学习
draft: false
---
# 一、简报

**难产的llama终于发布，几个月后的OpenAI仍然遥遥无期**

2025年4月6日，Meta宣布llama家族上新，宣布了三个llama 4系列模型，开源了两个：Llama 4 Scout、Llama 4 Maverick、Llama 4 Behemoth（规模最大，未开源）。

**这是模型摘要：**

- **Llama 4 Scout**，16位专家的170亿激活参数的多模态模型，**单个H100 GPU可运行**， 同类SOTA，并拥有10M（一千万）上下文窗口，并在广泛报道的基准测试中比 Gemma 3、Gemini 2.0 Flash-Lite 和 Mistral 3.1 有更好的表现
- **Llama 4 Maverick**，128位专家的170亿激活参数多模态模型，击败GPT-4o和Gemini 2.0 Flash，与DeepSeek-V3同等代码能力参数只要一半，主打与DeepSeek一样的性价比，**单个H100主机即可运行**。Maverick在广泛报告的基准测试中击败了 GPT-4o 和 Gemini 2.0 Flash，同时在推理和编码方面与新的 DeepSeek v3 取得可比的结果，而活跃参数不到一半。Llama 4 Maverick 以优于同类的性能成本比提供服务，实验聊天版本在 [LMArena](https://lmarena.ai/leaderboard) 上的 ELO 得分为 1417。
- **Llama 4 Behemoth**：2万亿（2880B）参数的超大超强模型，十六位专家，以上二者都由这个模型蒸馏而来；目前还在训练中；多个基准测试超过GPT-4.5、Claude Sonnet 3.7和 Gemini 2.0 Pro。

Meta还宣布，这些Llama 4模型标志着Llama生态系统新时代——**原生多模态AI**创新的开始。

接下来，我们一起看看本次Llama 4系列模型的创新点。

# 二、创新点解读

> 因为本次Llama4并没有发布技术报告，所有信源均来自于[官方博客](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
> 

## 1.全面转向MoE架构

Llama4系列是 Meta **首次**在旗舰模型中采用**专家混合 (MoE, Mixture of Experts) 架构**。与传统的“稠密”模型（每次计算都使用所有参数）不同，MoE 模型包含多个“专家”网络，对于每个输入（token），系统会动态地选择一小部分专家来处理。

**MoE 架构的核心优势在于计算效率**。在 Llama 4 中，每个 token 只激活总参数的一部分（称为“活跃参数”）。这使得模型在训练和推理时速度更快，计算成本更低，**在相同的计算资源（FLOPs预算）下，MoE 架构通常能比稠密模型达到更好的性能**。

例如，Llama 4 Maverick 使用了 **128 个路由专家** 和 **1 个共享专家**。每个 token 会被发送到共享专家，并同时被路由到 128 个专家中的一个进行处理。模型结构上采用了**稠密层和 MoE 层交替**的方式来进一步优化推理效率。

> Meta 还特别优化了 MoE 的并行化设计，以提高训练和推理速度。
> 

转向 MoE 是 Llama 4 实现更高性能和效率的关键一步，使得强大的模型（如 Maverick）能够在单个 H100 主机上运行成为可能，并为 Behemoth 这样的超大规模模型训练提供了基础。

![](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202504061344684.png)

## 2.原生多模态与早期融合

Llama4系列和Llama3系列最大的区别之一就是**Llama4系列模型是原生多模态模型**。

和市面上其他常见的多模态模型训练方式不一样，Llama4系列在训练时采用了**早期融合 (Early Fusion)**，也是 Llama 4 多模态能力的核心技术。与一些模型在后期才融合不同模态信息的做法不同，Llama 4 在模型主干网络的早期就将文本和视觉的 token **无缝集成**在一起处理。早期融合使得模型可以在包含大量**未标记**的文本、图像和视频数据的混合数据上进行联合预训练，从而学习到更深层次的跨模态关联。训练数据量是 Llama 3 的两倍多（超过 30 万亿 token）。

在架构方面，Llama 4 使用了基于 MetaCLIP 的改进版视觉编码器，这个编码器是与一个**冻结的 Llama 模型**一起单独训练的，目的是更好地将视觉信息“翻译”成 LLM 能够理解的表示。Llama4模型在预训练阶段接触了多达 48 张图像的输入，在实际应用（后训练测试）中可以处理最多 8 张图像，支持更复杂的视觉推理和互动任务。

> Llama 4 Scout 在图像定位方面表现突出，能够理解用户提示中涉及的视觉概念，并将模型的响应精确地锚定到图像中的特定区域，提升了视觉问答的准确性。
> 

中译中就是终于，finally支持其他家视觉模型同款的输出检测框功能。

## 3.超长上下文与架构创新

Llama 4 在处理长序列信息方面取得了突破性进展，尤其是 Llama 4 Scout，模型支持高达 **1000 万 (10M) token** 的上下文窗口，远超 Llama 3 的 128K 和业界普遍水平。Llama 4 Maverick 也支持 1M token。超长上下文可以解锁很多的模型应用场景，包括**处理和摘要极长的文档或多份文档**、**分析用户长时间、大范围的活动记录以实现深度个性化**和**对包含数百万 token 的庞大代码库进行理解和推理**等用例。

那么，为什么Llama4可以实现超长上下文呢？核心在于**架构创新 (iRoPE)**。

- **交错注意力层 (Interleaved Attention Layers)**：在 Llama 4 架构的部分层中，使用了**不带位置嵌入 (Positional Embeddings)** 的交错注意力机制。这是支撑“无限”上下文长度的核心设计。
- **推理时温度缩放 (Inference-Time Temperature Scaling)**：通过在推理时调整注意力机制的“温度”，增强了模型在处理超出训练长度的序列时的泛化能力（长度泛化）。
- **iRoPE 命名**：Meta 将这种架构称为 **iRoPE**。“i” 代表交错注意力层 (interleaved)，而 “RoPE” 指的是在模型**大多数**其他层中仍然使用的**旋转位置嵌入 (Rotary Positional Embeddings)**。

模型在“大海捞针 (Needle-in-a-haystack)”测试（在长文本中定位信息）和处理超过 1000 万代码 token 的 NLL（负对数似然）任务上都展示了良好效果。

![](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202504061353183.png)

## 4.训练与后训练优化

- **高效训练技术 (MetaP)**：Meta 开发了一种名为 MetaP 的新训练技术，能够更可靠地设置关键的超参数（如学习率、初始化规模），并使其能在不同模型尺寸、批次大小和训练数据量之间良好迁移。
- **FP8 精度训练**：为了在不牺牲质量的前提下最大化训练效率，Llama 4（尤其是 Behemoth）的预训练采用了 FP8 精度，配合 32K GPU 集群，实现了极高的计算吞吐量（单个 H100 GPU 达到 390 TFLOPs）。
- **中期训练 (Mid-training)**：在预训练和后训练之间增加了一个“中期训练”阶段，使用专门的数据集和新的训练方法来强化模型的核心能力，并扩展上下文长度。
- **精细化的后训练流程**：后训练流程被重新设计，以平衡多模态能力、推理和对话：
    - **改进的 SFT (Supervised Fine-Tuning)**：使用 Llama 模型作为“裁判”，过滤掉超过 50% 的简单数据，对更难的数据进行轻量级 SFT，避免过度拟合。
    - **在线强化学习 (Online RL)**：通过精心挑选的更难提示进行 RL 训练，显著提升性能。采用持续在线 RL 策略，在训练和利用模型之间交替进行，不断过滤和保留中等到困难的提示。
    - **轻量级 DPO (Direct Preference Optimization)**：在 RL 之后进行轻量级 DPO，处理与模型响应质量相关的边缘情况，平衡智能与对话能力。
- **教师模型蒸馏 (Behemoth -> Maverick)**：Llama 4 Maverick 的高质量部分得益于从 Llama 4 Behemoth 进行的**协同蒸馏 (collaborative distillation)**。Meta 开发了新颖的蒸馏损失函数，结合动态加权的软目标（教师模型的输出概率）和硬目标（正确答案）。利用 Behemoth 在预训练期间的前向传播计算结果作为 Maverick 的蒸馏目标，摊销了计算成本。
- **针对超大模型的后训练优化 (Behemoth)**：为 2T 参数的 Behemoth 进行后训练是巨大挑战。Meta 为此进一步优化，例如修剪了 95% 的 SFT 数据，采用轻量级 SFT 后进行大规模 RL，使用 pass@k 分析采样困难提示，动态过滤无用提示，以及完全异步的在线 RL 训练框架，将训练效率提升了约 10 倍。

## 5.多语言

Llama4在 **200 种语言**上进行了预训练，支持在超过 **100 种语言**上进行开源微调，且其中每种语言都有超过 **10 亿个 token** 的数据量，多语言标记 (token) 的数量比 Llama 3 多出 **10 倍**。

> 但中文能力……我们还是等即将到来的Qwen3吧。
> 

# 三、大声BB

## 1.端侧模型的头子都投了，未来主流路径应该还是以OpenAI和DeepSeek为代表的中心化全能MoE模型为主

之前的Llama1、Llama2、Llama3都是**Dense模型**，系列模型规模大致在7B-70B左右，端侧运行的可能性和优势比较明显，Llama在大模型社区内也长期占据着主流基座模型的开发者心智，吸引了一大批关注本地化、小型化部署的开发者。

但本次 Llama 4，尤其是 Maverick 和 Behemoth，明显是冲着云端、高性能计算去的。连 Scout 运行也需要 H100（虽然是单个）。这似乎印证了一个趋势：尽管端侧有需求，但要追求最前沿的性能、最长的上下文、最强的多模态能力，**目前还得靠数据中心里的高性能超算**。**如果连Llama这种曾经的“端侧之光”都开始主攻需要H100级别算力才能跑起来的模型，那未来端侧/云端模型之争的天平，似乎在加速倒向OpenAI和DeepSeek所代表的云端中心化、追求极致性能和通用能力的MoE模型路线**。本地化部署的未来可能更多在于这些大模型的“轻量版”或者特定任务的微调版本，而非前沿基础模型本身。

## 2.**全面拥抱 MoE，大模型技术架构趋同**

之前 Mistral 是最早在大模型方面让 MoE 架构火出圈的，Google 的 Gemini 也用了 MoE（至少Gemini 1.5 pro是MoE），基本上大模型主流赛道的旗舰模型都是MoE架构（OpenAI、Google、Grok、DeepSeek、Qwen）。现在 Meta Llama 4 全系列拥抱 MoE，基本说明想把模型参数和能力再往上堆一个数量级，同时还要兼顾（相对）效率，**MoE 是目前业界公认的最优解之一**。这也意味着模型训练和推理的复杂性又上了一个台阶，对基础设施和优化技术的要求更高了。稠密模型也许在某些特定场景还有优势，但在追求通用智能和规模效应的路上，MoE 似乎已成必选项。**大家的技术路线越来越像了，卷的方向也越来越一致：更大的参数规模（通过MoE实现）、更强的多模态、更长的上下文。** 这也意味着，大模型领域的“军备竞赛”进入白热化阶段，比拼的是谁能更快地训练出更大、更优的 MoE 模型，以及谁拥有更强的工程优化能力和数据处理能力。

## 3.其实说创新也没多创新

我们来细数一下Llama 4的创新点：

- **MoE 架构？**主流玩家早都已经换完了，Llama 4 是**首次**在旗舰模型用，但不是第一个吃螃蟹的。
- **原生多模态？** GPT-4V、Gemini 珠玉在前，大家都在做。早期融合 (Early Fusion) 算是一个实现细节上的优化，但多模态本身不是新概念。
- **超长上下文？** Llama 4 Scout 的 10M 确实是目前最长，iRoPE 架构是其实现的关键，这算是一个不错的工程创新。但追求长上下文这个**方向**本身，也是行业趋势，之前Gemini、Minimax，甚至Qwen也在这方面进行过探索，并且成果都挺显著的。
- **训练优化？** FP8 训练、RLHF/DPO 流程、模型蒸馏，这些都是当前大模型训练的常规操作或渐进式改进（让我们感谢DeepSeek）。MetaP 可能是 Meta 内部提效的法宝，但对外界来说，更多是工程细节的打磨。

Llama 4本次相当于整合了市场上最热门、最有效的技术方向，优化并且**在理论上**推向了新的规模和性能高度，尤其是在开源模型领域再次树立了标杆。

> 为什么是理论上呢，我们下一节就说
> 

但要说开创了全新的、颠覆性的技术路线，好像还谈不上。更像是站在前人（或者说同行）的肩膀上，做了一次**工程和整合能力的极致展现**，把现有的 "SOTA 配方" 调得更猛、融合得更好了。

## 4.我们再退一万步讲，好像模型性能也就那样

Llama 4 Scout 和 Maverick 的性能确实亮眼，在各自的细分领域（轻量级多模态 SOTA、高性价比 MoE）做到了领先。Scout 能在单 H100 上跑还带 10M 上下文，这很实用；Maverick 性价比看齐 DeepSeek V3，用一半的活跃参数达到相似性能，这也很厉害。

但仔细看，Maverick 对标的是 GPT-4o 和 Gemini Flash，打赢了固然可喜，但考虑到参数规模和 MoE 架构的加持，似乎也在情理之中。与 DeepSeek V3 打平手，也说明大家技术水平在同一梯队，差距在毫厘之间。

至于那个真正对标 GPT-4.5、Sonnet 3.7、Gemini Pro 的**终极大杀器 Llama 4 Behemoth**，目前还处于“期货”状态，仍在训练中。它宣称的性能超越听起来很诱人，但毕竟还没正式发布和接受公开检验。

所以，目前发布的 Llama 4 是很强，尤其是在开源社区和特定性能/成本区间内极具竞争力。但它带来的更多是**一次强有力的追赶和局部超越**，以及开源领域的新标杆。要说它带来了颠覆性的、让所有人惊呼“AI 又进化到了全新纪元！”的那种体验代差，至少从目前发布的 Scout 和 Maverick 来看，似乎还没到那个程度。更像是一次**意料之中、情理之内的强力迭代**，证明了 Meta 依然是顶级玩家，但整个 AI 领域可能进入了一个性能提升边际效应递减、需要更长时间积累才能迎来下一次质变的阶段。

## 5.好话说完了，接下来该社区反馈了

前面分析了 Llama 4 的诸多亮点和技术实力，Meta 的官方博客和各种基准测试也描绘了一幅美好的图景。然而，模型发布后，社区的实际体验和反馈却带来了一些不同的声音，甚至可以说是“杂音”。

根据我目前掌握的信源，**似乎Meta大力宣传的Llama 4 Scout 和 Maverick在编码领域并不如他们宣称的和打榜的那么好**，有很多开发者质疑官网下载的maverick模型和lmarena竞技场的仿佛不是一个模型，无论是代码能力，还是写作能力，甚至输出文风都完全不一样，官网下载的模型明显远远不如竞技场。

**在Linux do论坛，[有开发者质疑竞技场和实际发布模型差距过大，货不对板](https://linux.do/t/topic/536079)：**

> 我理解一定程度的差异，但是二者差异大到了让我开始怀疑甚至是meta放错了代码，给错了模型。几乎不像是一个模型。我无意贬低这个模型，竞技场的体验是不错的，文风很好，文笔很棒。虽然逻辑差点，指令跟随性差点。可是这个发布出来的，我很难形容这是竞技场里我体验到的，我实际上上面的图片也可以证明，二者几乎“能力完全不对等”
> 

> 再补一句，reddit上有人测试竞技场的知识量和or发布的知识量似乎是不相同的，很多竞技场的是知道的，但是放到or渠道就不知道了（我没有实测，在此放上贴图，如果有老哥实测欢迎贴出来）
> 

![](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202504061447509.png)

**这意味着什么？**

如果社区的这些反馈具有普遍性，那问题就比较微妙了：

1. **基准测试与实际体验脱节？** LMArena 上的高分，以及官方报告中的优异表现，可能是在特定的、高度优化的环境下取得的。这可能是特定的系统提示 (System Prompt)、推理参数设置，甚至是未公开的微调版本。普通开发者下载模型后，在自己的环境中复现这种“巅峰状态”可能非常困难，导致心理落差巨大。
2. **模型版本混乱或误导？** 最坏的情况，就像那位开发者猜测的，是否存在发布的版本和用于打榜、展示的版本并非完全一致？这会严重影响 Meta 开源的可信度。虽然大公司不太可能犯这种低级错误，但社区的强烈质疑值得关注。
3. **对“编码能力”的定义差异？** Meta 宣称 Maverick 在编码上媲美 DeepSeek V3，但社区反馈不佳。这可能是因为评测基准 (Benchmarks) 侧重的方面（例如代码生成、补全）和开发者实际工作流中更看重的方面（例如复杂逻辑理解、Debug、项目级代码理解）存在差异。基准测试高分不一定等于实际开发体验好。
4. **需要特定的“启动咒语”？** 有些模型需要非常精巧的 Prompt Engineering 才能发挥最佳性能。也许下载版的 Llama 4 需要特定的提示技巧，而这些技巧并未在发布时充分说明？

# 四、总结

本次Llama 4系列发布**充斥着仓促气息**。尽管Meta带来了技术上的诸多更新，如首次在**自己的旗舰模型**中采用MoE架构、实现原生多模态早期融合、通过iRoPE架构将上下文窗口推向千万级别等，但与Llama 2、Llama 3发布时那种“开源之光”技惊四座、甚至在某些方面引领潮流的感觉相比，Llama 4更像是一次**快速整合与追赶**。

从**技术创新**角度看，Llama 4的核心亮点大多是对行业现有SOTA（State-of-the-Art）方向的跟进和优化，而非开创性的突破。全面转向MoE是对DeepSeek、Mistral等先行者的追随；原生多模态是业界标配；超长上下文虽有工程创新（iRoPE），但方向本身也是Gemini、月之暗面、Qwen等早已布局的领域；训练和后训练技术（FP8、MetaP、RL/DPO流程优化、蒸馏）更多是渐进式改进和工程能力的体现。相比之下，一些竞品在注意力机制（如MoBA、NSA、Lighting Attention）或RL方法（如DeepSeek R1）上展现出的探索性更强。Llama 4给人的感觉是，Meta利用其强大的工程和算力资源，将现有“配方”快速组合并推向了新的规模，目的是先**跟上第一梯队**，弥补之前（如坚持Dense模型）的战略判断。

在**性能与市场反响**方面，虽然Scout和Maverick在基准测试上表现亮眼，甚至在特定指标上超越了GPT-4o等强力对手，但并未带来革命性的体验代差。更重要的是，发布后社区迅速出现的“货不对板”的质疑声浪——开发者反映实际下载的模型在编码、写作等方面的能力远不如LMArena竞技场版本——给这次发布蒙上了一层阴影。这种体验上的脱节，无论源于版本差异、特定配置要求还是评测方法的局限，都削弱了Llama 4本应带来的冲击力，并加剧了其“仓促发布”的印象。

从**战略和时机**来看，Llama 4的发布也显得有些微妙。全面拥抱MoE更像是对先前路线的一次“纠错”。选择在周末发布，以及竞争对手（如Gemini研究员、千问负责人）在社交媒体上颇具玩味的调侃，都指向一种可能性：Meta或许是为了避开下周可能出现的更强竞品（传闻中的DeepSeek、Qwen、DeepMind新模型），而选择抢先发声。这与Llama 3时期作为领先者的自信姿态形成了鲜明对比。同时，在行业对推理模型和AI Agent需求高涨的背景下，Llama 4并未优先推出此类模型，也未发布能在本地轻松运行的小尺寸版本，这让部分开发者感到失望。

**总而言之**，Llama 4的发布无疑巩固了Meta作为AI顶级玩家的地位，它依然是开源社区一个强大的新选择，特别是在理论性能和某些特定能力（如超长上下文）上设立了新标杆。然而，这次发布更像是一次**防御性和追赶性的迭代**，是Meta在快速变化的市场格局中，为稳住阵脚、跟上步伐而进行的一次必要但略显仓促的技术整合与“秀肌肉”（尤其是通过Behemoth的训练规模）。其“追赶”的意味强于“引领”。真正的考验，或许在于**尚未完全亮相的Behemoth**能否真正兑现其超越GPT-4.5等的承诺，以及Meta如何有效回应社区反馈、弥合基准与实际体验的鸿沟。从曾经的开源领跑者到如今奋力追赶的身影，Meta AI未来的道路依然充满挑战。

# 参见

[1] [Meta官方关于 Llama 4 发布的技术博客](https://ai.meta.com/blog/llama-4-multimodal-intelligence)

[2] 量子位：LIama 4发布重夺开源第一！DeepSeek同等代码能力但参数减一半，一张H100就能跑，还有两万亿参数超大杯

[3] 机器之心：Meta深夜开源Llama 4！首次采用MoE，惊人千万token上下文，竞技场超越DeepSeek

[4] 硅星人Pro：1000万上下文+2880亿参数的Llama4，却让DeepSeek们松了一口气