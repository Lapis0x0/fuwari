---
title: 闪电注意力的又一次胜利：Minimax-M1 技术报告浅读
published: 2025-06-21
description: "工程上的又一次胜利，商业实践中难言满意。"
image: ""
tags: ["模型考古学"]
category: 深度学习
draft: false
---

距离上一篇[「模型考古学」系列解析 Minimax-01](https://www.lapis.cafe/posts/ai-and-deep-learning/minimax-01-report/) 已有3个月的时间。当时我们深入探讨了它如何通过“闪电注意力（Lightning Attention）”与标准 Softmax 注意力混合的架构，在长上下文处理上取得突破，为大模型在效率与性能之间寻求平衡提供了一条极具前景的工程路径。

2025年6月，Minimax 团队再次为我们带来了其最新力作——Minimax-M1。如果说 Minimax-01 是对「闪电注意力能否在商业级模型中立足」的一次成功探索，那么 Minimax-M1 的技术报告则高调宣告了这一路线的**又一次重大胜利**。

Minimax-M1 基于此前研发的 Minimax-Text-01（即 Minimax 01 模型），**总参数规模为 4560 亿（456B），其中激活参数为 459 亿，采用了 32 个专家模块（即活跃参数量为 45.9B）**。在模型结构上，延续了此前的设计思路：每七个采用闪电注意力（Flash Attention）的 Transnormer 模块后，接入一个带 Softmax 注意力的 Transformer 模块，以此在计算效率与表达能力之间取得平衡。

在计算性能方面，相较于 DeepSeek R1，M1 在生成 64K token 时的 FLOPs 消耗降低超过 50%；在 100K token 任务中，FLOPs 仅为对方约 25%。这意味着 M1 在推理阶段和大规模强化学习训练中都具备显著的算力优势。

得益于闪电注意力机制和与 MiniMax-Text-01 架构的连续性，M1 原生支持最长达 100 万 token 的上下文窗口，是 DeepSeek R1 的 **8 倍**，在目前已公开的大模型体系中处于**量级领先**地位，**且长上下文性能非常不错**，对于需要处理长文本输入、链式推理、多轮交互等复杂现实任务的场景而言，M1 的能力构成了天然优势。

本来还打算顺带看看 kimi-dev-72b 的技术报告，结果发现他们还没放出来……目前只知道该模型是基于 Qwen 2.5 72B 微调的，从架构出发点来看，纸面性能应该不差。

> 毕竟，基于 Qwen 2.5 72B 进行微调，无论如何也不会差到哪里去就是了。

# 一、论文速读：架构与工程艺术

## 1. 核心架构：混合注意力的再确认与深化

M1 的根基依然是我们熟悉的 **混合注意力（Hybrid Attention）** 架构。报告再次确认了这一设计的核心思想，并给出了更具体的实现细节：

*   **7:1 的黄金配比**：模型中，每七个采用闪电注意力（Lightning Attention）的 Transnormer 模块之后，会跟随一个使用标准 Softmax 注意力的 Transformer 模块。
*   **分工明确的优势互补**：
    *   **闪电注意力**：作为线性注意力的一种高效变体，它负责处理绝大部分的计算，其计算复杂度不随序列长度 `n` 增长，从而为百万级上下文窗口和长程生成提供了可能。这是效率的保证。
    *   **Softmax 注意力**：虽然计算成本高昂（$O(n^2)$），但它被保留下来作为关键的“检索”和“信息整合”单元。报告中有一个生动的比喻：Softmax 注意力就像在推理过程中 **“翻阅书籍”**，它通过重新计算和回顾所有历史信息，确保了模型能够精确地从长上下文中检索和保留关键信息，这对于需要上下文学习（In-Context Learning）的任务至关重要。

## 2. 训练流程与工程创新：从预训练到强化学习

M1 的训练分为三个核心阶段：持续预训练、监督微调（SFT）和大规模强化学习（RL）。其中，RL 阶段是本次报告的重中之重，也是工程创新最密集的部分。

### 2.1 持续预训练与 SFT：奠定坚实基础

*   **数据为王**：在 Minimax-Text-01 的基础上，追加了 **7.5 万亿 token** 的高质量数据进行持续预训练。特别强调了 **STEM（科学、技术、工程、数学）、代码、书籍及推理相关数据的比例被提升至 70%**，为模型注入了强大的逻辑推理内核。
*   **平滑的长上下文扩展**：在将上下文从 32K 扩展到 1M 的过程中，团队发现“过度激进”的扩展会导致梯度爆炸。为此，他们设计了一套**分四个阶段、更平滑的扩展策略**，这揭示了超长上下文训练的真实挑战远非调整几个参数那么简单。
*   **SFT 的目标**：SFT 阶段的核心目标是向模型注入**基于反思的思维链（CoT）模式**，为后续的强化学习阶段“对齐”好行为模式，使其更容易学会复杂的推理。

### 2.2 强化学习的“心脏”：CISPO 算法

这是本次报告**最核心的算法创新点**。传统的 RL 算法如 PPO/GRPO 在训练时，会通过“裁剪”操作来稳定训练过程。但 Minimax 团队发现，这种操作存在一个致命缺陷：

*   **问题所在**：对于一些在推理中至关重要的、但初始概率较低的“关键思考节点”词元（token），例如“然而”、“我们来复查一下”、“啊哈，我发现了……”等，它们的**重要性采样权重（Importance Sampling Weight）** 会很高。在 PPO/GRPO 中，这些词元往往因为权重过高而被**直接裁剪掉**，导致模型无法从这些最关键的“灵光一现”中学习。

*   **CISPO 的解决方案**：为了解决这个问题，他们提出了 **CISPO（Clipped Importance Sampling Policy Optimization）** 算法。其核心思想是：**不再裁剪词元本身，而是裁剪其重要性采样权重 `r_i,t(θ)`**。

    $$
    \hat{r}_{i,t}(\theta) = \text{clip}(r_{i,t}(\theta), 1 - \epsilon_{\text{low}}, 1 + \epsilon_{\text{high}})
    $$

    这意味着，**所有词元都会参与梯度更新**，保证了学习的完整性；但那些权重过高的词元，其影响力会被平滑地限制在一个合理范围内，从而保证了训练的稳定性。

*   **效果显著**：报告中的实验（图2）表明，相比 GRPO 和 DAPO，CISPO 不仅性能更优，而且**训练速度提升了近一倍**，仅用 50% 的训练步数就达到了与 DAPO 相当的性能。

### 2.3 混合架构下的 RL 工程挑战与解决方案

他们坦诚地分享了在混合架构下进行大规模 RL 训练时遇到的具体困难和解决方案：

1.  **计算精度失配问题**：他们发现，RL 训练时（使用训练核）和实际生成时（使用推理核）计算出的词元概率存在显著差异（图3左），这种不一致性严重阻碍了奖励的提升。
    *   **定位问题**：通过逐层分析，发现误差主要来源于输出层 LM head 的高幅值激活。
    *   **解决方案**：将 **LM 输出头的计算精度从 FP16 提升至 FP32**，成功地将训练与推理的概率相关性从 0.9x 提升至 0.99x（图3右），解决了这个棘手的工程难题。

2.  **优化器超参数敏感性**：他们发现 AdamW 优化器在默认配置下会导致训练不稳定。
    *   **解决方案**：通过细致实验，最终确定了一组非标准的超参数（`β1=0.9, β2=0.95, eps=1e-15`），从而稳定了梯度，保证了训练的顺利进行。

3.  **病态重复生成问题**：在 RL 训练中，模型有时会陷入“病态的长重复响应”，产生大量无意义内容，这会威胁模型稳定性。
    *   **解决方案**：开发了一种**基于标记概率的启发式早期截断机制**。当检测到模型连续生成高概率词元（进入重复循环的标志）时，就提前终止生成。

## 3. 长程思维训练：将生成长度推向 80K

在完成了基础 RL 训练后，团队进一步将模型的**生成长度**从 40K 扩展至 80K。他们发现一个“负样本长度增长过快”的问题。在 RL 中，失败的尝试（负样本）往往比成功的推理（正样本）链条更长，导致模型在长程生成时不成比例地积累了过多负梯度，最终导致“模式崩溃”（后半段生成内容退化为胡言乱语）。

解决方法：
1.  **样本级损失与 token 级归一化结合**：缓解正负样本长度不均衡带来的负面影响。
2.  **动态降低梯度裁剪阈值**：进一步增强长程生成的稳定性。
3.  复用前述的**重复模式检测机制**。

通过这一系列精细的调优，M1 成功地掌握了在 80K 长度上进行稳定、高质量思维链生成的能力。

# 二、实际性能：But at what cost?

以下跑分来自于公众号：大模型观测员 的私人数据集，我认为他的测评是第三方里相对可信的。

>他在知乎上也有账号：[链接](https://www.zhihu.com/people/toyama)

![image.png](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/202506211814981.png?imageSlim)

我们可以看到榜单中 Minimax-M1 的得分并不理想，其极限分数（45.97）和中位数分数（32.18）与 DeepSeek R1、Qwen3-235B 等第一梯队模型存在显著差距。-42.84% 的中位数分差也暴露了其性能的**不稳定性**。

诚然，M1 的成本（8元/百万 token）和上下文长度（1M）确实是其巨大的市场优势，完全兑现了技术报告中关于“高效扩展推理计算能力”的承诺，但代价是：

### 1.严重依赖「暴力枚举」，缺乏巧思

测评指出，M1 在面对需要思维技巧的问题时（如 #24 算24点），倾向于“快速放弃，转为遍历求解”。这与我们在第一部分分析的 **RL 训练策略** 可能存在关联。为了训练模型在 80K 长度上稳定生成，团队投入了大量精力解决“模式崩溃”和“负样本过长”等问题。这可能导致模型被优化得更擅长 **“把话说完、说长”**，而不是 **“把问题想对、想巧”**。

>24年的AI六小龙进入25年，更新速度都慢下来，阶跃星辰和智谱在4月发布推理模型，kimi做推理模型更早，但放出API是在5月初。除了提前出局的01和百川，最后一家MiniMax也进入推理时代。然而这4家的推理模型都遇到共性问题，即普遍的推理超时和死循环。尤其智谱Z1最初版本死循环率达到惊人的80%，在复杂问题上几乎不可用。
>
>反而是传统互联网企业和老牌AI企业，百度，腾讯，商汤，他们的推理模型在这一轮推理竞赛中暂时力压六小龙。

我能想到的一个相对合理的解释是：作为新兴的独角兽企业，AI 六小龙普遍缺乏传统互联网公司积累多年的海量数据，尤其是高质量、结构化的长程推理数据，这恰恰是推理类大模型最为依赖的训练材料。它们既没有坚实的数据基础，也缺乏系统性的工程积累，更没有足够的人力资源投入到精细化的数据标注与清洗中。因此，在长程推理能力方面，这些模型整体上往往不如传统互联网大厂所推出的产品。

> 当然，DeepSeek 是一个例外。

### 2.惊艳的底层能力与不稳定的上层逻辑

测评中提到了一个非常有趣的现象：在 #11 岛屿问题和 #18 字符迷宫中，M1 展示了精准的字符级处理能力和空间推理潜力，甚至能解决许多强模型都失败的复杂案例。这恰恰印证了其混合注意力架构中 **Softmax 注意力模块** 的作用——强大的“翻阅”和“检索”能力，让它能精确记住长序列中的细节。

然而，它却在更简单的问题上翻车，或者在具备正确推理路径的情况下犯下低级错误。这表明模型的**底层能力（细节检索）与上层能力（逻辑整合）之间存在一定的脱节**。那 1/8 的 Softmax 注意力层确保了信息不丢失，但那 7/8 的闪电注意力层在传递和处理这些信息的过程中，可能未能有效地构建起稳固、连贯的推理框架。

### 3.推理速度与超时间隔

平均 294 秒的耗时和高达 53% 的超时率，**是其“暴力”推理模式在实际应用中的直接后果**。模型需要消耗海量的计算时间和 token 才能完成一次“遍历”，这使得其宣称的“80K 最大推理额度”在面对复杂问题时变得难以触及。用户等了半天，最终换来的可能是一次超时或一个不尽人意的结果。

>国产模型普遍完成复杂任务耗时过长，其实也有国内用不到最先进黄卡的原因在。最先进的那一两代黄卡跑模型推理的速度是真的爽


# 总结

综上所述，Minimax-M1 是一次在工程实现层面极具胆识且初步成功的探索。它向我们展示了：**通过混合注意力架构，确实可以构建出兼具超长上下文和低推理成本的商业化大模型**。然而，这种架构也并非没有代价——至少在当前版本中，M1 似乎是以牺牲部分核心推理能力为交换，换取了在上下文长度与计算开销这两个维度上的极致表现。这种取舍，或许正是混合架构在现阶段无法回避的结构性权衡，也为我们理解多种技术路径下的能力分布提供了一个极具参考价值的案例。

在实际使用中，M1 的 API 表现难言令人满意：推理速度稳定在约 30 tokens/s，整体响应偏慢，生成内容在逻辑深度与连贯性上也较为稀疏，常常有“纯凑数”的感觉。就应用场景而言，它更像是为**具备一定技术实力、能够负担小规模 H100 集群的企业客户**所打造，适用于自部署场景下的工具调用与超长文档处理。而对更广泛的开发者群体和高强度交互类任务来说，M1 当前的表现仍显不足。

尽管如此，其在注意力机制上的创新依然令人印象深刻。能在工程上实现百万级上下文窗口、显著降低推理 FLOPs，并保持相对稳定的运行表现，已是一种非常值得肯定的科研突破。换句话说，M1 的意义或许不仅在于“能用”，更在于“可行”——**它为探索混合注意力架构的大模型开辟了现实路径，也为后续优化和迭代提供了扎实的工程样本**。
