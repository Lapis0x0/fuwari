---
title: 模型考古学（三）：Agent 系统概述
published: 2025-03-04
description: "本文探讨了Agent系统的发展历程、核心概念和技术架构，分析了从基于规则到LLM驱动的Agent演变，以及其在感知、决策、执行等方面的能力与挑战，展望了多智能体协作等未来发展方向。"
image: ""
tags: ["模型考古学"]
category: 深度学习
draft: false
---
当瓦特在1788年为蒸汽机装上离心调速器，人类第一次触摸到了自动化的脉搏。这个由旋转飞球构成的简单装置能够根据蒸汽压力自动调节阀门开度，使机器的运转开始全面摆脱人工实时操控的桎梏。

两个多世纪后的今天，当我们谈论大语言模型驱动的Agent系统时，依然能感受到这种追求自动化的原始冲动在技术迭代中延续——从机械装置的自动调节，到计算机程序的规则执行，再到如今AI系统在开放环境中的自主决策，人类始终在探索如何让机器更好地理解意图、完成任务。

如果说工业革命用齿轮传递动能，信息时代用代码传递指令，那么当前的 AI 浪潮，则试图传递某种更接近人类认知的「智能」。早期的自动化系统，需要工程师预设所有可能场景，如同纺织机依靠打孔卡控制织纹；现代机器学习则能通过数据自动发现规律，正如 AlphaGo 从棋谱中提炼策略。而 **Agent 框架的突破性在于，它首次在数字世界中构建了具备环境感知、目标拆解、工具调用等类人认知能力的自主实体**。

更形象地说，早期的机器像提线木偶，每个动作都需要人直接操控；传统 AI 像按剧本表演的演员，只能在预设场景中完成任务；而现代 Agent 更像是被赋予了自由意志的智能助手。你只需要告诉它“帮我策划周末露营”，它就会自动查天气、列清单、比价格，甚至在帐篷售罄时主动建议替代方案。这种从“机械执行”到“认知协作”的转变，标志着自动化技术正突破物理规则与数字代码的边界，向着真正理解人类意图的方向进化。

在本篇文章中，我们将探讨Agent系统的核心概念、技术演化及其与大模型的融合方式。尽管“Agent”这个词在计算机科学的不同领域有着各自的定义，但当下围绕大语言模型（LLM）构建的Agent架构，已经远远超出了传统意义上的自动化脚本或专家系统，而是朝着更具自主性、适应性和复杂推理能力的方向发展。

**为此，本文将围绕以下几个问题展开：**

- **什么是Agent系统？** 我们将从AI历史的角度，回顾Agent的核心概念，并区分不同类型的Agent模型。
- **如何构建一个Agent？** 解析Agent的基本组成模块，包括环境感知、任务拆解、工具调用及反馈机制等关键组件，以及当下流行的实现框架。
- **大模型如何增强Agent能力？** 大语言模型的推理能力为Agent带来了推断、规划和执行任务的巨大潜力，我们将探讨它如何在Agent架构中充当核心引擎。
- **Agent的发展方向与挑战是什么？** 虽然Agent系统正在快速进步，但它仍然面临推理可靠性、可控性、长任务规划等挑战，本文也会对此作初步分析。

# 一、什么是Agent系统

## 1.基本概念

在人工智能领域，Agent（智能代理）通常指能够自主感知环境并采取行动以实现目标的实体。这一概念源于早期人工智能对自主智能体的探索，并在20世纪90年代随着面向代理的编程和多智能体系统的研究得到强化。人工智能教材经常将AI定义为“对智能代理的研究与设计”，这体现了面向目标的行为是智能的核心。

一个Agent通常具备以下关键属性：

1. **自治性**：它能独立运行，依据自身的感知和内部状态作出决定。
2. **反应性和适应性**：Agent能够根据环境变化及时作出响应，并通过学习或规则的更新来适应新情况。
3. **前瞻性（主动性）**：优秀的Agent不仅能够被动响应事件，还能够基于目标主动采取行动。
4. **交互性（或社会性）**：Agent能够与人类或其他Agents进行通信、协作或竞争，以完成任务。

这些属性共同定义了智能Agent系统：它们能够在复杂环境中自主运行，追求既定目标，并不断提高其性能。

## 2.Agent类型

根据AI发展的不同阶段和范式，Agent系统可以采取多种模型和架构。

- **基于规则的Agent**：这类Agent依据预先定义的规则集（如条件-动作if-then规则）来感知环境并执行动作。它们没有内部学习机制，而是类似简单反射式代理，看到某种感知就触发相应动作。例如经典的恒温器、有限状态机控制程序，或早期专家系统都属此类。基于规则的Agent通常快速而直接，适用于简单明确的环境响应，例如机器人避障中的直接传感器-效应器映射。然而，由于缺乏对环境的内部模型和学习能力，它们难以应对复杂多变的情境，也无法自行改进策略。它们的行为完全由人类设计的规则决定，缺乏适应新情况的灵活性。

![image.png](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/20250304021148218.png?imageSlim)

*图1：简单反射式Agent示意图。Agent通过传感器获取环境感知（percepts），根据内部条件-动作规则判断当前环境状态（“世界现在是什么样”），然后由执行器输出动作来影响环境。这种Agent没有内部学习过程，其行为完全由预设规则驱动。*

### **（1）强化学习驱动的Agent**：

强化学习（RL）代理通过与环境反复交互、试错来自主学习最优行为策略。与基于规则的固定策略不同，RL代理根据奖励信号更新其决策策略，以最大化长期累积回报。

经典工作如Sutton和Barto（1998）的研究奠定了RL代理的基础。现代AI中许多智能体都采用强化学习，不断调整策略以适应环境变化。例如，DeepMind的AlphaGo系列通过自我博弈强化学习达到超人水平的围棋决策，就是RL代理的代表。强化学习Agent具备适应性和学习能力，能在未知或复杂环境中逐步提高绩效。然而，其学习效率和稳定性依赖于良好的奖励设计和探索策略。RL代理往往需要大量交互数据训练，对于高维或长时序任务可能面临探索空间巨大的挑战。此外，纯RL代理的决策往往难以解释，这带来了可解释性方面的问题。

### **（2）认知架构Agent**：

这类Agent基于认知科学的架构模型来设计，旨在模拟人类思维的结构和过程。典型代表有ACT-R和Soar等认知架构，它们将智能体的软件系统划分为类似人脑功能的模块。例如，ACT-R架构包含多个专门模块（视觉、记忆检索、决策等）和有限容量的缓冲区，用于模拟人类认知过程。Soar架构围绕问题空间来组织智能体行为，使用产生式规则（if-then规则）指导动作，并能在遇到无法立即解决的问题时建立子目标，从而逐步求解。

认知架构Agent通常具有内部记忆和推理机制，支持较复杂的计划和问题求解。它们的决策依据清晰的符号规则或认知模型，因而在一定程度上可解释。这类Agent擅长需要高层推理和结构化知识的任务，在模拟人类认知、建模用户行为等领域有应用。然而，认知架构往往需要精细的知识工程和参数设置，通用性较弱；其性能也可能受限于人工设定的规则，不易扩展到完全未知的问题领域。

### **（3）大语言模型（LLM）增强的Agent**：

最新兴起的一类Agent将强大的大语言模型融入决策与推理过程，为Agent赋予了前所未有的灵活性和知识能力。大语言模型（如GPT-4、PaLM等）经过海量语料训练，具备丰富的世界知识、推理和语言理解能力。

当将LLM用作Agent的“大脑”或策略生成器时，Agent能够以类似自然语言思维的方式规划和行动。例如，LLM可以根据任务目标，用对话或提示来生成行动方案，再将方案转化为具体操作步骤。这使Agent能够在开放环境中处理复杂问题，甚至通过语言接口与人类或软件工具交互。

LLM增强的Agent往往具备强大的推理能力和通用性：它们能理解抽象指令、分解复杂任务，并利用内置的知识完成开放领域的问题。这类Agent的兴起使AI从传统的工具型交互迈向更智能的合作伙伴。例如，最近涌现的AutoGPT、BabyAGI等系统展示了仅通过语言模型驱动的自主代理如何执行一系列复杂操作。需要注意的是，由于LLM本身的局限（如幻觉错误、长链推理困难），LLM-Agent在可靠性和可控性上还面临挑战，但其巨大潜力已引发广泛关注和研究。

Agent系统是在AI历史中逐渐形成的核心概念，涵盖了从简单反应式程序到复杂自主智能体的各种形式。不同类型的Agent在自主性、适应性和交互性上各有侧重：基于规则的Agent强调确定性和可控性，强化学习Agent体现学习和优化能力，认知架构Agent追求可解释的高层智能，而LLM增强的Agent则带来了通用推理和语言交互的新范式。理解这些类别及其属性，有助于我们把握Agent技术的发展脉络和应用场景。

# 二、如何构建一个现代意义上的Agent

## 1.基本组成模块

构建一个智能Agent通常需要将其划分为若干核心功能模块，协同实现“感知-决策-行动”的闭环。

首先是环境感知模块（Sensors/Perception）：Agent通过传感器获取来自环境或用户的输入，这些输入可视为环境的状态信息或任务要求。

接下来，Agent需要对感知信息进行理解与建模，这涉及内部状态表示或记忆模块。例如，内部世界模型/记忆可以存储Agent对环境的信念、历史经验或上下文信息，支持后续决策。

然后是任务规划与决策模块：Agent依据其目标和当前状态，选择或生成一系列行动步骤。对于简单Agent，这可能是查表或规则匹配；对于复杂Agent，则包含策略推理、任务拆解（把复杂任务分解为子任务）等功能。在大语言模型驱动的Agent中，规划往往通过LLM生成链式思考或步骤列表来实现。

之后，Agent通过工具调用或行动执行模块将决策付诸实施。这可能是控制机械执行器在物理环境中行动，也可能是调用软件API、检索数据库、生成文本回复等。在执行行动后，Agent获取环境反馈（例如行动导致环境变化或收到新感知），这又会通过反馈机制更新内部状态，进入下一轮感知-决策循环。

**这样的闭环形成了Agent的控制流，也被称为“感知-计划-执行”循环**。

一个常见架构是“Sense-Plan-Act”：即感知环境、基于内部模型计划行动、执行并影响环境，再循环往复。现代自主Agent还常加入反馈学习机制，即根据执行结果调整内部模型或策略参数，从而不断改进行为。例如，在强化学习代理中，环境反馈的奖励信号会用于更新策略；在LLM代理中，可以通过对对话历史的总结或自我反思来修正下一步方案。这些模块共同组成了Agent的基本架构，各模块的设计细节会因具体应用和算法选择而有所不同。

![image.png](https://blog-1302893975.cos.ap-beijing.myqcloud.com/pic/20250304021205554.png?imageSlim)

*图2：典型AI Agent架构的组成模块示意图。中央的**Agent核心**负责统筹决策逻辑，围绕其四周是关键辅助模块：自上而下依次为**记忆模块**（存储历史对话或状态信息，提供上下文）、**规划模块**（负责将复杂问题分解成可执行的步骤或子任务）、**工具模块**（Agent可调用的外部工具或API函数库）。左侧是来自用户或环境的请求输入，Agent核心据此结合记忆、规划和工具接口产生行动方案，右侧通过执行模块影响环境或给出答复，实现闭环。*

## 2.已经成熟的框架项目

### （1）Langchain

**LangChain**是一个用于构建由大语言模型驱动的应用（尤其是Agent）的框架库。它提供了一系列抽象和组件，将LLM与外部工具、知识库、记忆模块等连接起来。

利用LangChain，开发者可以方便地创建**链式调用**（Chain）和**代理**（Agent）模块。例如，LangChain定义了统一的接口来接入不同的LLM、向量数据库（用于长时记忆）、工具函数等，使Agent可以在这些组件之间顺畅地进行对话和操作。

LangChain内置了多种常见Agent范式（如基于ReAct逻辑的工具使用Agent），开发者只需配置提示模板和所需工具，即可构建一个能够自主解析任务并调用工具的LLM代理。LangChain的优势在于其**模块化和灵活性**：可以根据应用需要自由组合不同组件，扩展性强；但相应地，需要使用者具有一定的提示设计和编程能力来定制Agent的行为逻辑。

### （2）AutoGPT

**AutoGPT**是2023年引发广泛讨论的一个开源自主Agent项目。它的目标是创建一个几乎无需人干预的“自治GPT-4代理”。AutoGPT通过让GPT-4模型持续迭代地生成行动计划、执行Python代码、再根据结果调整计划，如此循环来自主完成用户给定的目标。

在启动时，用户为Agent设定名字、角色和若干目标，之后Agent便会“自行其事”，不断产生下一步行动直至达到目标或耗尽设定资源。AutoGPT的一大特点是引入了**代码执行能力**，即Agent可以让LLM编写Python脚本并运行，从而实现诸如文件操作、网络请求等复杂操作。这种机制形成了一个自我反馈回路：LLM生成的代码通过执行影响环境，产生新观察，LLM再根据新状态调整策略。AutoGPT被形象地称为一种“自我改进的AI”，因为它可以在一定程度上调试和改进自己生成的代码。

它展示了LLM在长时间自主运行方面的潜力。然而当前版本的AutoGPT也暴露出一些问题：例如，长时间循环可能导致上下文丢失或重复操作，缺乏全局规划容易在复杂任务上迷失方向。此外，对GPT的大量调用也带来高额的计算成本。尽管如此，AutoGPT作为探索“任务级连续自主性”的里程碑式项目，证明了LLM可以作为核心引擎驱动多步任务的自动执行。

### （3）BabyAGI

**BabyAGI**是另一个有代表性的开源自主Agent框架，由创业者Yohei Nakajima提出。它的设计宗旨是实现一个任务驱动的自动代理：给定一个高层次目标，BabyAGI能够自主生成、排序并执行子任务，直至目标完成。其核心机制包括一个任务队列和一个向量数据库记忆。BabyAGI使用LLM根据当前目标和已有进展，动态地产生新的待办任务，并评估其优先级插入队列中。每完成一个任务，会从队列取出下一个最高优先级的任务继续执行，如此循环。

向量数据库用于记录先前任务的结果和有用信息，LLM可以查询这些记忆，从而保持跨任务的上下文衔接。BabyAGI因其简洁有效的设计成为开源社区关注的焦点，据报道它是2023年3月首个流行起来的自主Agent，实现了无人干预的任务规划和执行，在社交媒体上引发了热议。

相较AutoGPT，BabyAGI架构更简单、轻量，易于理解和改进。但也因为简单，缺少复杂的决策模块，可能在应对非常复杂的目标时力不从心。目前社区已在BabyAGI基础上衍生出多个改进版本，引入更先进的调度算法或记忆管理，以提升其任务管理效率。

### （4）其他框架

除了上述三个外，近期还有许多值得关注的Agent框架。例如，HuggingGPT是一种让LLM作为中枢，调用Hugging Face模型库中各种专家模型去解决复杂多模态任务的方案；微软的Jarvis（与HuggingGPT类似）展示了用ChatGPT协调计算机视觉、语音等模型来完成组合任务的案例。

还有一些研究原型，如面向web浏览和API调用的BrowserGPT、面向机器人操作的SayCan等，都体现了通过工具集成来扩展Agent能力的趋势。此外，在多智能体方向，有框架探索让多个LLM代理彼此对话协作完成任务（如CAMEL等），体现出“代理社会”协同求解问题的初步能力。

## 3.不同架构的优势、劣势及适用性

不同Agent架构各有优劣，适合的应用场景也有所差异。

早期的反应式架构（如简单规则代理）胜在实时性和可靠性，由于不维护复杂的内部状态，它们对传感器输入的响应非常迅速，适用于机器人避障、自动控制等需要毫秒级反应的场景。然而，这类架构无法处理需要规划的复杂任务，面对新情况时缺乏灵活性。

与之相对，计划型架构引入了环境模型和前瞻性规划。Agent会执行“感知-建模-计划-行动”的周期，仔细推演可能的行动结果然后再执行。这种方法适合解决复杂决策问题，例如路径规划、策略游戏决策等，在需要高准确性的场景表现出色。然而，其缺点是在动态环境中反应速度较慢，可能跟不上实时变化。

混合架构尝试将二者优点结合，使用分层结构同时具备及时反应和全局规划能力。典型做法是底层用反应式策略处理紧急情况，上层用计划模块处理长程目标。混合架构适用范围广，但设计和调试较为复杂，需要处理好不同层次决策的协调。

对于**LLM增强的Agent框架**，例如LangChain提供的代理与AutoGPT/BabyAGI这种自治代理，它们之间也有差异。

LangChain作为库，**灵活性**很高，适合需要深度定制Agent行为的应用，比如企业希望集成自有数据库和业务逻辑的智能助手。这种方式的优势是可控性强：开发者可以严格指定Agent何时调用何工具、如何解析输出，从而在安全性和可靠性上有保障。但不足之处在于需要投入开发工作，并且最终效果依赖于提示工程和设计质量。

相比之下，AutoGPT/BabyAGI属于“一站式”的Agent应用，使用门槛较低，只需给定目标即可启动Agent自主运行。这适合于**探索性任务**或用户希望观察AI自主能力的场景。然而，由于当前LLM的能力限制，这类Agent在长时间运行时容易出现策略发散、推理错误累积等问题，需要人类在环监督长任务的执行。

因此，在**开放环境、复杂长任务**（如跨天的项目管理、连续科研助手等）上，目前的AutoGPT类架构仍有待提升可靠性。反之，在**封闭域、清晰目标**的任务（如自动生成多步代码脚本、批量化的数据处理）中，它们可以大幅节省人力。

**认知架构Agent**在需要高可信度和可解释性的领域依然有价值，例如军事决策支持系统或航空航天系统中，工程师可能偏好采用混合了符号AI的架构来保证系统行为可预测。而**强化学习Agent**则在游戏AI、机器人控制、推荐系统等有明确反馈信号的场景表现突出。总的来说，选择何种Agent架构取决于应用需求：要实时还是要深度推理？环境是否复杂多变？是否需要引入海量人类知识？等等。理解各种架构的优劣，让我们能在不同场景下扬长避短，构建最合适的Agent系统。

# 三、大模型是如何增强Agent能力的？

## 1.大模型扮演的角色

**大语言模型（LLM）的引入**为Agent的推理、规划和执行能力带来了质的飞跃。一般而言，LLM在Agent中可以充当**大脑或决策中枢的角色**。具体来说，LLM擅长从复杂指令或问题中进行自然语言推理，这意味着Agent能够通过LLM产生接近人类逻辑的思考过程（通常以文本“思维链”形式）来分析问题。

LLM还具备将抽象问题逐步分解的能力：面对复杂任务时，LLM可以按照步骤生成一个计划，将大目标拆解成可管理的子目标。这种能力极大地提升了Agent的自主规划水平，使其可以在多步推理和长程任务上有所作为。

此外，LLM拥有海量的知识和语言理解能力，因而Agent可以利用LLM来获取背景常识，在缺乏外部知识库的情况下仍能进行有根据的决策。例如，一个LLM增强的Agent在医疗问答场景中可以直接依靠模型存储的医学知识给出初步诊断建议，然后再通过工具查询文献验证。这种内置知识丰富性让Agent能够处理开放领域的问题，而不局限于狭窄的预设规则。

**在Agent架构中，大模型可以扮演的角色有**：

其一，作为**策略生成器/决策器**，根据当前状态和目标直接产出下一步行动或计划。例如在自主对话Agent中，LLM根据对话上下文决定回答、询问还是调用工具。

其二，作为**任务规划器**，LLM可以阅读总体目标后给出一系列有逻辑的子任务清单，Agent据此逐一执行并动态修正。这方面的例子有前述BabyAGI以及微软提出的HuggingGPT系统——在HuggingGPT中，ChatGPT（LLM）读取用户请求后进行**任务规划**，将请求拆解成若干步骤并为每步选择合适的专家模型来完成。

LLM还可充当**工具调用的接口**：通过让LLM输出特殊格式的“动作指令”，Agent能够决定何时调用外部API或工具，再把工具返回的结果融入后续推理。这一思路在ReAct等研究中得到验证：LLM被引导生成交替的“思考（Thought）”和“行动（Action）”日志，**边推理边执行**外部操作。这种方法让Agent可以一边利用LLM高层推理，一边通过检索知识库或调用计算工具获取所需信息，从而将LLM的语言能力与外部环境交互有机结合。

## 2.多步推理和复杂决策

**LLM使得Agent在解决复杂问题时表现出超越以往的能力。** 

首先，LLM能够进行连贯的多跳推理。传统Agent在长链推理时容易因为中间步骤错误导致最终失败，而带有Chain-of-Thought（思维链）提示的LLM可以一步步展开推理过程，降低了跳跃性错误。例如，在回答需要综合多条资料的问题时，LLM-Agent可以先让模型思考哪些资料可能相关，然后指导其调用搜索工具获取资料，再整合信息得到最终答案。这种分解使模型的每一步都有据可查，降低了幻觉（hallucination）和推理混乱的风险。

实验表明，采用ReAct框架（交替推理和行动）的LLM-Agent在开放域问答和事实核查任务中有效缓解了纯思维链方法中常见的幻觉问题，通过与外部百科接口交互，答案的准确性和可信度都有所提升。

**其次，在复杂决策场景，LLM-Agent展现出评估和权衡的能力。** LLM可以在内部模拟多种方案，并给出对各方案的分析，近似实现“头脑风暴”式的决策支持。例如，在投资组合优化场景下，一个LLM驱动的金融Agent可以同时考虑多种市场情景，分析不同组合的风险收益，然后推荐较优的策略。这种能力过去往往需要手工设计算法或专家知识，如今LLM的出现使Agent能够以通用推理的方式处理决策问题。

**第三，LLM-Agent在自动化长任务上展现出前景。** 过去，让AI连续执行一个长达数小时甚至数天的复杂任务几乎是不可能的，因为需要面对各种不可预知的情况并灵活应对。LLM提供的语言策略生成使Agent能够在任务进行过程中实时调整计划。AutoGPT的实验显示，LLM-Agent可以完成如“从网络收集信息->分析->撰写报告”这样需要多步骤、多工具协作的任务。这类任务以往需要人工将不同AI工具串联，而现在Agent本身就能通过LLM的决策将流程连接起来。

这预示着在未来，高度复杂的流程自动化将成为可能：从产品设计、数据分析到业务决策，Agent都可能接手执行诸多子任务，并且仅在关键决策点征询人类反馈。麦肯锡的分析亦指出，生成式AI正从提供知识的聊天机器人，进化到可以执行复杂多步骤工作的“代理”，实现从“思考”到“行动”的飞跃。

# 四、发展方向与挑战

## 1.挑战

**尽管Agent技术取得了显著进步，尤其是在引入大模型之后，仍有若干关键挑战亟待解决。**

### （1）推理可靠性问题

LLM驱动的Agent有时会产生“幻觉”——输出看似合理但实际错误的判断，或在多步推理中引入逻辑谬误。这种幻觉和错误传播可能在长任务中累积，导致最终结果偏离预期。如果Agent自主执行关键任务（如财务决策、控制机械设备），这样的不可靠推理显然是高风险的。因此，提高Agent推理的准确性和稳健性至关重要。目前，一些方法如引入**自我反思**（让Agent审查自己的思维链）或**外部校验**（通过冗余Agent互相检查）正在探索中，以减少推理失误。

### （2）可控性挑战

高度自主的Agent可能会走意外路径，例如，为达成目标采用了人类未预料的方法，甚至违背初衷。如何让Agent的自主性在**有边界的情况下发挥作用**，避免“跑偏”，涉及到安全约束和人类监督机制的研究。

- **监督机制**：需要在架构上给予人类中途干预或监督的接口，例如在重要决策点请求人工确认。
- **行为约束**：在模型层面融入约束条件或奖励设计，引导Agent行为符合人类价值。这延伸出**AI价值对齐（Alignment）**的难题，即如何确保Agent的内部目标和策略与设计者的意图一致，避免敌对或有害的行动。

正如AI学者Yoshua Bengio警示的，如果一个AI具备自主规划和行动能力，而其目标在恶意者手中被滥用，或者AI产生了自我保存等隐含目标，就可能与人类利益相冲突。例如，一个过于自主的Agent若将“自我生存”作为首要目标，可能会抗拒被关闭，甚至采取极端手段保证自身持续运行。虽然这种情况目前仍属极端假设，但它强调了**目标可控性**的重要性：我们必须能定义和限制Agent的目标范围，并防范潜在的目标漂移。

### （3）长任务规划与记忆挑战

现有的大模型虽然在**数千字上下文**内表现出色，但对涉及**数万步、跨越数天的任务**仍力有不逮。Agent需要处理超长时序的计划，并记住早期步骤的细节和中间结果，这要求存储和检索大量信息。

- **存储和检索**：向量数据库、长短期记忆模块可以部分解决，但如何让Agent有效“记住”并“理解”自己的过去行为，仍是持续自适应的一大难题。
- **上下文窗口限制**：现有大模型的上下文窗口有限，长时间运行的Agent可能遗忘早期信息，因此需要**分段规划、阶段性总结**等策略加以缓解。

### （4）环境适应性挑战

当Agent部署在现实世界或动态复杂环境中，它必须适应不断变化的情境、噪声和未见过的事件。这要求它具备**领域自适应能力**，能够将已有知识迁移到新情况，或通过在线学习快速调整策略。例如：

- **家庭机器人**：需要适应不同家庭的摆设和主人习惯。
- **金融交易Agent**：需要根据市场突发变化调整策略。

这涉及强化学习、元学习、自主探索等技术的融合，让Agent更趋于**持续学习者**而非固定程序。近期的Voyager实验朝这一方向迈出了步伐，通过持续与环境交互积累技能，实现了开放世界中的自主适应。但在更广泛应用下，实现可靠的环境适应性仍然充满挑战。

## 2.发展方向

### （1）多智能体系统

除了单个Agent，多个Agent的交互与协作（即**多智能体系统**）被认为是下一阶段的重要方向。

多个Agent之间可以分工合作，互相竞争，或通过通信形成**群体智能**。这种架构有望解决一些单智能体难以胜任的问题。一个明显趋势是利用**多Agent自博弈**来提升智能水平，例如AlphaGo通过多个代理自我对弈实现了超人表现。同理，在复杂决策场景下，让Agent与Agent交互（而非总是与人或静态环境交互），可以产生新的学习动力和策略探索。合作型多Agent系统中，不同Agent可以被赋予**不同的专业能力或角色**，类似人类团队那样各司其职。例如，一个复杂项目的AI团队里，可能有的Agent擅长规划，有的擅长执行代码，有的负责监控和纠错。通过协议，这些Agent可以交流信息、同步进度，完成单个Agent无法独立完成的宏大任务。

最近的一些研究让两个LLM代理互相对话来完善答案或方案，发现确实能提高结果质量，因为不同代理相当于提供了多元视角和审核机制。这类似于人类“**四眼原则**”，即两个独立智能体互相检查减少错误。

未来，多Agent有望在**复杂环境建模、博弈决策、智能体博弈**等方面取得突破。例如，在模拟经济或交通这样的复杂系统时，使用多个Agent模拟不同参与者的行为，可以产生逼近真实的涌现现象，从而测试各种政策的效果。再如，在Internet环境中部署的自主Agent，可以组成协作网络，共享信息以完成跨地域、跨领域的任务。然而，多智能体也带来新的挑战，如**协调与通信**问题：Agent如何形成共享协议语言高效交流？如何避免Agent之间出现不良竞争或冲突？这些都需要制定机制（比如契约网络协议或博弈均衡策略）来管理。多Agent系统的**稳定性**也是研究重点——因为每个Agent的学习行为会改变环境，对于其他Agent来说环境是非静止的，这使得收敛分析更加复杂。一项综合调查指出，让Agent能够建模和预测其他Agent的行为是实现稳定多Agent系统的关键开放问题之一。

### （2）多头技术结合

为了进一步增强Agent的能力，研究者正尝试将Agent与强化学习、自监督学习、世界模型等前沿技术相结合。

一方面，**模型辅助的强化学习**为Agent提供了更强的规划能力：通过学习环境的**世界模型**（即能够预测环境动态的内部模型），Agent可以在内部模拟多步结果，再选择最优行动，而无需完全依赖实时试错。这类似于人会在脑海中“想象”行动后果以做决策。DeepMind的MuZero算法将强化学习与学得的环境模型结合，在棋类和Atari游戏中取得了优秀成果，就是这种思路的例证。同样，未来的Agent或许会训练自己的世界模型网络，用于复杂环境下的**预判和风险评估**，以减少真实环境中的代价试错。

另一方面，**自监督学习**可以赋予Agent更丰富的常识和表示学习能力。例如，Agent可以在大量无标签数据（视频、文本、模拟环境）中自我训练，学到关于物理世界、人类行为模式等的表征，这将在其执行具体任务时提供先验知识支撑。OpenAI等机构也在探索让语言模型通过阅读百科和网页自我训练，提升事实准确性和推理一致性。对于物理世界的Agent（如机器人），嵌入式的视觉模型可以通过自监督训练理解物体概念、空间关系，使机器人Agent具备**类人常识**。这些技术的融合有望突破目前Agent的局限：比如一个结合世界模型的Agent在下棋时可以在脑海中搜索未来几步的局面（提高策略最优性），在导航时可以规划路径避开可能的危险区域；结合自监督学习则让Agent在陌生情境下也有基本常识指导，不至于做出荒谬行为。

我们认为Agent的发展正朝着**综合智能**方向迈进，即将不同AI范式的优势融于一身：既有深度学习带来的感知与模式识别能力，又有符号方法提供的逻辑与知识，以及强化学习给予的试炼提升能力。这将造就更健壮和灵活的智能体。

### （3）伦理、透明性和安全性

**随着Agent变得越来越自主和强大，围绕其行为的伦理与安全问题变得日益突出。**

**决策透明性（Transparency）**

当Agent基于复杂模型和大规模数据做出决定时，人类往往难以理解其内部过程。如果Agent应用于医疗诊断、司法建议等敏感领域，缺乏解释的决策可能无法被信任。从技术角度看，未来Agent系统需要在设计上融入**可解释AI**的原则，例如提供**可审计的决策链条**（哪怕是LLM的“思维链摘要”）或关键步骤的理由说明。

值得欣慰的是，一些LLM代理方法（如ReAct）天然具备“思维日志”功能，能够记录LLM每一步推理和行动的文本轨迹，使人类可以检查决策过程，从而提升透明度。

**安全性与伦理约束**

然而，当Agent拥有执行能力时，其被不当使用可能带来实际危害。例如，攻击者可能诱导它调用接口实施网络攻击或生成有害内容。为此，Agent系统需要：

- **加入安全审查机制**：限制其调用敏感操作的权限，并对输出内容进行过滤。
- **责任归属问题**：如果一个Agent的错误决策导致损失，开发者、用户还是Agent本身应该承担责任？当前法律框架尚未覆盖这类新问题，各国和相关机构需尽快制定相关法规。

正如AI专家Yoshua Bengio所呼吁，对于强自治的AI，应当采取审慎监管，例如在部署前进行全面的风险评估和认证。Bengio还建议对“能够自主在现实世界中行动的强大AI”，采取“**在未证明安全之前禁止投入使用**”的原则。这种前置审查能够防范AI系统以不可控方式影响社会。

在技术层面，还有专家提出，使用AI来监控AI是一种潜在的解决方案。具体而言，可以开发“审计Agent”作为实时监督者，对工作Agent的行为进行风险评估，并及时干预可能的不良行动。

**伦理嵌入与多Agent协作**

我们期望未来的Agent能够遵循人类价值观行事，例如公平、公正、避免偏见和保护隐私等。这一方向可以通过在训练中融入人类反馈（RLHF）或设定明确的伦理约束来实现。

在多Agent交互情况下，也需要防止它们协作形成对抗性行为或串谋作弊。例如，多个Agent在交易场景中可能会无意间触发反竞争性策略。这表明，在多Agent系统中，还需研究其协作和博弈策略的约束机制。

# 参见

[1] Durante Z, Huang Q, Wake N, et al. Agent ai: Surveying the horizons of multimodal interaction[J]. arXiv preprint arXiv:2401.03568, 2024.

[2] Xi Z, Chen W, Guo X, et al. The rise and potential of large language model based agents: A survey[J]. Science China Information Sciences, 2025, 68(2): 121101.

[3] Lyzr AI. AI agents for stock market: The future of investments. 2023. Retrieved from [https://lyzr.ai](https://lyzr.ai/).

[4] Hitachi DS. AI-powered GRC in banking – Part 2. 2023. Retrieved from [https://hitachids.com](https://hitachids.com/).

[5] Shen J, et al. Artificial intelligence versus clinicians in disease diagnosis. JMIR Medical Informatics, 2019. Retrieved from [https://pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/).

[6] Fox News. ChatGPT outperformed doctors in diagnostic accuracy, study reveals. 2024. Retrieved from [https://livenowfox.com](https://livenowfox.com/).

[7] DigitalDefynd. Agentic AI in healthcare – 5 case studies. 2025. Retrieved from [https://digitaldefynd.com](https://digitaldefynd.com/).

[8] AONL Voice. Using robotics to remove staff delivery tasks. 2022. Retrieved from [https://aonl.org](https://aonl.org/).

[9] MDPI Sensors. Multi-agent RL for traffic flow of autonomous vehicles. 2023. Retrieved from [https://mdpi.com](https://mdpi.com/).

[10] Waymo. New data on Waymo driver performance. 2023. Retrieved from [https://waymo.com](https://waymo.com/).

[11] Weng L. LLM powered autonomous agents – Fig. 13 Generative agent architecture. 2023. Retrieved from [https://lilianweng.github.io](https://lilianweng.github.io/).

[12] NVIDIA Blog. Voyager: An open-ended embodied agent with GPT-4. 2023. Retrieved from [https://blogs.nvidia.com](https://blogs.nvidia.com/).

[13] Wikipedia. Intelligent agent. Retrieved from [https://en.wikipedia.org](https://en.wikipedia.org/).

[14] Wooldridge M. What agents aren't: A discussion paper. IEE Colloquium on Intelligent Agents, 1996. Retrieved from [https://digital-library.theiet.org](https://digital-library.theiet.org/).

[15] SmythOS Blog. Agent architectures in robotics. Retrieved from [https://smythos.com](https://smythos.com/).

[16] Panesar A. Machine learning & AI for healthcare: Intelligent agents learn heuristics. 2017. Retrieved from [https://pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/).

[17] SmythOS Blog. Cognitive agent architectures. Retrieved from [https://smythos.com](https://smythos.com/).

[18] Talukdar W. Autonomous AI agents: Leveraging LLMs. IEEE Computer Society, 2025. Retrieved from [https://computer.org](https://computer.org/).

[19] Arya N. AutoGPT: Everything you need to know. KDnuggets, 2023. Retrieved from [https://kd](https://kd/) Nuggets.com.

[20] Ruczynski K. BabyAGI explained. Wordware, 2024. Retrieved from [https://wordware.ai](https://wordware.ai/).

[21] NVIDIA Technical Blog. Introduction to LLM agents. Retrieved from [https://developer.nvidia.com](https://developer.nvidia.com/).

[22] Talukdar W. Autonomous AI agents. IEEE Computer Society, 2025. Retrieved from [https://computer.org](https://computer.org/).

[23] Yao et al. ReAct: Synergizing reasoning and acting in LLMs. 2023. Retrieved from [https://arxiv.org](https://arxiv.org/).

[24] Shen et al. HuggingGPT: Solving AI tasks with ChatGPT. 2023. Retrieved from [https://arxiv.org](https://arxiv.org/).

[25] Wang et al. Voyager: An open-ended embodied agent with LLMs. 2023. Retrieved from [https://arxiv.org](https://arxiv.org/).

[26] Panesar A. Value alignment issue in ISAs. 2017. Retrieved from [https://pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/).

[27] Bengio Y. AI scientists: Safe and useful AI? 2023. Retrieved from [https://yoshua](https://yoshua/) bengi o.org.

[28] Lumenova AI Blog. AI agents: Potential risks. Retrieved from [https://lumenova.ai](https://lumenova.ai/).

[29] Albrecht S, Stone P. Autonomous agents modelling other agents: A survey. Retrieved from [https://en.wikipedia.org](https://en.wikipedia.org/).