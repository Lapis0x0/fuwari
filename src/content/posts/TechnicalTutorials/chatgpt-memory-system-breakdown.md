---
title: 浅谈ChatGPT的记忆实现机制 兼论工程端记忆设计
published: 2025-05-25
description: "本文系统梳理了 ChatGPT 的记忆系统实现机制，并探讨了工程实践中不同层次的“记忆”设计思路与权衡方法，兼具技术性与现实可操作性。"
image: ""
tags: ["模型考古学"]
category: 技术教程
draft: false
---

要想研究ChatGPT这个产品的“记忆”功能实现机制，我们就必须要从大模型本身的“记忆”到底是个什么东西开始说起。

在我们的传统的，人类视角的认知里，“记忆”意味着信息的持久储存和可随时调用，但这一常识在神经网络中往往并不成立。大语言模型（LLM）本身其实并**不具备“记住某个具体事实”或“反复调用某段对话”的内建机制** 。它们所谓的“记忆”，更多体现在**参数记忆** 的层面——即通过反复训练，将大量的语料信息固化在数百亿甚至万亿级的参数中，从而形成一种对语言结构、事实知识乃至人类行为模式的“潜在记忆”。

虽然这种参数记忆赋予了大模型前所未有的知识广度，但其本身的信息管理是静态的，只能反映训练阶段所接触到的信息，**无法根据用户的即时输入动态调整，也无法在多轮对话之间保持状态**。这也意味着，大模型在默认状态下是**无记忆、无连续性**的。为了让用户在使用中获得“ChatGPT 记得我说过什么”的体验，系统必须引入额外的机制来补全这种能力的缺失。最直接的方式，便是**上下文管理（context management）**。

所谓上下文管理，是指在一次交互过程中，**将用户与模型之间的多轮对话，打包成一个“对话上下文窗口”（context window）一并输入模型**，从而模拟出“记得过去对话”的效果。这种机制并非模型主动“记住”了内容，而是每一次调用模型时都“重新喂给它过去发生的事情”。

>这也是为什么很多初次使用大模型api的用户会惊讶为啥每一轮对话所耗费的额度会呈现滚雪球式增长，因为如果你不限制上下文总窗口和轮数的话，系统需要在每一次请求中携带之前的全部对话内容，自然而然就会账单爆炸了。

上下文管理是目前 LLM 实现短期记忆的核心方法，但它也存在明显的限制：**窗口大小有限（目前主流模型支持的最大上下文为128k tokens），每次输入的上下文越长，推理成本也越高**；同时它无法实现真正的“长期记忆”——例如跨会话的状态保留、用户偏好的追踪、个人资料的学习等。这就引出了 ChatGPT 的另一个核心设计：**用户级的“记忆系统”**。

为了给用户更好的，更智能、温情的对话体验，这个系统试图实现的是**对用户信息的持久存储与调取**，补上“上下文窗口”无法承担的那部分“跨会话记忆”。但它不是模型本身的能力，而是产品层的一项“外挂功能”，通过在后端数据库中记录“用户告诉过模型的信息”，并在合适的时候动态注入这些内容到上下文中，从而模拟出模型“记得你”的效果。

在下文中，我们将围绕 ChatGPT 的这一“类人记忆”系统展开分析，探讨其设计原则、实现逻辑与当前的能力边界。

# 一、发展演进梳理

我们可以通过下面这张表格来来一窥ChatGPT记忆系统的演进：

| **日期/时期**  | **功能/更新**     | **核心功能**               | **用户层级**               | **关键用户控制方式**                  |
| ---------- | ------------- | ---------------------- | ---------------------- | ----------------------------- |
| 2024年之前    | 仅上下文窗口        | 会话内回忆，受限于tokens数量      | 所有用户                   | 除提示工程外基本无                     |
| 2024年早期    | 自定义指令         | 用户定义的持久性指南             | 所有用户                   | 管理自定义指令                       |
| 2025年4月之前  | “已保存的记忆” (显式) | 用户明确告知的事实性信息存储         | Plus, Pro, 免费版 (仅限此功能) | 查看/删除已保存的记忆                   |
| 2025年4月10日 | “聊天历史”参考 (隐式) | 自动从所有过去聊天中提取洞察以供回忆     | Plus, Pro              | 切换“已保存的记忆”/“聊天历史”开关，临时聊天，存档聊天 |
| 2025年4月之后  | 双重记忆系统        | 结合显式用户指令和隐式AI学习的综合回忆机制 | Plus, Pro              | 同上，并可询问“你记得关于我的什么？”           |
>2025年4月10日，OpenAI宣布，ChatGPT现在可以参考用户所有的过去聊天记录，以提供更加个性化和相关的回应。官方通过@OpenAI和萨姆·阿尔特曼的推文发布了这一消息 ，并在FAQ文档中提供了更多细节 。萨姆·阿尔特曼称之为一项“出人意料的强大功能”，并指出它预示着“AI系统将在你的一生中了解你，并变得极其有用和个性化” 。这一变革使得ChatGPT从主要依赖显式指令或会话内上下文，转变为一个能够基于用户全部互动历史持续学习和自我定制的模型 。

更新后，ChatGPT的记忆系统以双重方式运行：
- **“已保存的记忆” (Saved Memories)**：用户明确要求ChatGPT记住的细节（例如，“记住我是素食主义者”）。这可视为先前显式记忆系统的演进。
- **“聊天历史”参考 (Chat History Reference)**：ChatGPT从过去的对话中自动收集洞察，以改进未来的互动，即使这些信息未被明确保存。这是更新、更全面的记忆层。 OpenAI建议用户将关键信息通过“已保存的记忆”功能来固定，因为“聊天历史”并非逐字记录所有细节，而是综合提炼洞察。
## 底层机制
既然是 OpenAI 出品的功能，其实现方式自然不可能像早期开源项目那样简单粗暴——例如直接将上下文丢进向量数据库，待用时再暴力召回。相比之下，OpenAI 构建了一个**由系统自动维护的动态用户画像机制**，并在每次新对话开始时，将相关信息注入到系统提示中，以实现类人“长期记忆”的体验。
- **`Model Set Context` (模型设定上下文)**：包含用户明确“保存的记忆”，并附有时间戳。例如：“1. [2025-05-02]. 用户喜欢冰淇淋和饼干。” 
- **`Assistant Response Preferences` (助手回应偏好)**：基于过去互动风格，指示ChatGPT应如何组织其回应。例如，用户可能偏好XML、JSON等结构化格式。此部分通常带有一个`Confidence` (置信度)标签 。
- **`Notable Past Conversation Topic Highlights` (过往对话主题重点)**：记录了以往对话中的高级别主题摘要，以保持未来讨论的连续性。例如，用户对AI漏洞或脚本编写的兴趣。此部分也包含`Confidence`标签 。
- **`Helpful User Insights` (有用的用户洞察)**：聚合了关于用户的具体事实信息，如姓名、职业、研究兴趣、博客地址等。
- **`Recent Conversation Content` (近期对话内容)**：存储数量有限（约40条）的近期聊天摘要，包含用户输入的消息，但不包括AI的回应，这可能是为了控制数据量和降低注入风险。时间戳的详细程度随对话的新近度而变化。
- **`User Interaction Metadata` (用户互动元数据)**：自动生成的账户使用信息，包括不同模型使用比例、账户年龄、设备类型、平均对话深度、常用意图标签（`intent_tags`）以及 UI 偏好等。

ChatGPT 会在持续使用中逐步形成对用户的**抽象性理解与偏好建模**。当用户输入新的请求时，模型并不会“无脑加载”所有历史内容，而是通过一系列筛选和匹配算法，挑选与当前上下文最相关的信息注入提示中。对于用户保存的记忆条目，系统采用语义索引机制，在嵌入向量空间中计算相关性，通常会检索出最相关的 5 到 20 条数据，用于增强模型当前的应答能力。

接下来，我将展示一份经过去敏处理的 ChatGPT 老用户档案（序号对不上是我删了那个条目），帮助大家直观了解这一用户画像系统究竟是如何“描绘”用户的。

## 1.模型设定上下文与助手回应偏好 
```
{
  "Model Set Context": {
    "1": "用户在论文写作中常用的统计学软件是Stata，编程语言是Python。",
    "2": "在未来帮助用户调试代码时，需要指出具体需要修改的地方，而不是提供整个项目代码。",
    "3": "用户有一个GitHub仓库的上游仓库是DIYgod/RSSHub。",
    "4": "用户这周准备把谢林的近代哲学史看完。",
    "5": "用户制作日语五十音Anki卡片，使用拼音表示假名发音。",
    "6": "用户喜欢太空策略游戏《群星》。",
    "7": "请记住，要用中文回复用户的问题。",
    "9": "用户最近在看ARK Invest的《The Big Ideas》研报，对其中的区块链方面特别感兴趣。",
    "11": "用户计划为自己的博客开设一个新系列，名为「暗涌」，主要侧重于创投领域的专业分析。",
    "13": "用户现在居住在某城市。",
    "14": "用户想写一篇关于「美联储政策工具与美国宏观经济指标解读」的博客。",
    "15": "用户在加密货币市场主要关注超买和超卖指标。",
    "17": "用户喜欢咖啡。",
    "22": "用户希望博客的文章链接格式为  目录下，后缀形式类似于 ai--deep-learning/tech-review-openai-o3/，即分类+子分类+文章slug 的路径结构。",
  "Assistant Response Preferences": {
    "1": "用户倾向于结构化但具对话性的语气，常要求在金融、AI和经济主题上平衡清晰与深度。",
    "2": "用户经常润色和迭代内容，尤其是在技术或报告写作时非常注重精准性。",
    "3": "用户在探讨 AI 架构、金融建模等主题时喜欢深入技术细节，但在一般事实查询时更偏好简洁总结。",
    "4": "用户在讨论理论时重视其实际应用，常追问其在市场、投资等情境下的运用。",
    "5": "用户喜欢互动和迭代的交流，不偏好长篇独白，除非主题要求深入探索。",
    "6": "用户有时喜欢轻松风格，但即使在非正式场合也期待有见地的回应。",
    "7": "用户偏好实用、以研究为基础的视角，倾向质疑过于泛泛的说法。",
    "8": "用户在处理技术文档、金融分析等密集内容时喜欢带有标题或编号的条理清晰的信息。",
    "9": "用户在探讨 Web3、区块链等趋势时表现出怀疑，倾向质疑其真实影响力。",
    "10": "在故障排查或自动化建议中，用户偏好直接执行路径，而不是抽象理论。"
  }
}
```
## 2.过往对话主题重点
```
  "Notable Past Conversation Topic Highlights": "略，为后续添加。",
": {
    "1": "在2025年2月至3月的对话中，用户讨论并分析了大型语言模型，尤其是 DeepSeek V3 和 R1。他们特别关注如多Token预测（MTP）、多头潜在注意力（MLA）和专家混合（MoE）等技术。用户还探讨了FP8训练和混合注意力机制等优化方法，以提升长上下文处理能力。同时还分析了强化学习驱动的模型微调（如DS R1）以及监督微调（SFT）阶段与RL微调方法的区别。",
    "2": "信息被隐去",
    "3": "在2025年初，用户深入研究了RAG（Retrieval-Augmented Generation）技术，撰写了一篇详尽博客，解释其在提升LLM检索能力中的作用。他们探索了融合检索与生成的架构以提高准确率，包括微软的GraphRAG，并研究了知识检索管线与向量搜索优化。重点在于实际应用和处理长尾查询、保持事实准确性的效率提升。",
    "4": "2025年2月至3月期间，用户研究了金融风险分析，尤其是VaR（在险价值）、历史模拟技术与组合层面的金融预测。他们将统计与机器学习模型应用于资产定价，并讨论了用于比特币价格预测的时间序列方法，还探讨了美联储政策工具如何影响宏观经济指标。",
    "5": "用户对ARK Invest的《Big Ideas 2025》报告进行了深入分析，聚焦AI、自动物流、生物科技、机器人与能源转型等投资主题。他们详细讨论了AI驱动的自动化，包括AI代理、决策系统与云端基础设施，认为核能可能成为满足AI推理需求的可行能源解决方案。其评论聚焦于战略投资影响与宏观经济趋势。"
  }
}
```
## 3.有用的用户洞察

```
  "Helpful User Insights": {
    "1": "用户的学校和职业（已隐去）",
    "2": "用户最近的工作历程（已隐去）",
    "3": "用户对金融市场、投资和宏观经济非常感兴趣。",
    "4": "用户正在撰写一个系列博客，聚焦于金融与技术主题，尤其是投资策略、宏观经济趋势和AI应用。",
    "5": "用户具备技术背景与AI应用实践经验，尤其是Python编程，曾开发投资相关自动化工具和区块链金融应用。",
    "6": "内容已隐去",
    "7": "用户积极研究先进AI模型与Transformer架构，尤其关注如RAG、MLA、MoE等优化技术，并撰写相关技术文档。",
    "8": "用户熟练掌握多种编程语言，尤其是Python，并具备机器学习、数据库管理与API集成经验。",
    "9": "用户对哲学，特别是形而上学与德国观念论（如莱布尼茨与斯宾诺莎）有浓厚兴趣。",
    "10": "用户是资深游戏玩家，喜欢《原神》和《群星》等策略与幻想类游戏。（这里是因为我每次都喜欢问如何评价原神来测试模型的回复策略）"
  }
}
```

## 4.用户交互元数据
```
  "User Interaction Metadata": {
    "1": "系统检测到用户当前所在地区为美国（可能因使用VPN不准确）",
    "2": "在过去的对话中，有28%使用了gpt-4o，1%使用了gpt-4-5，21%使用了o3，43%使用了gpt4t_1_v4_mm_0116等其他模型。",
    "3": "用户平均消息长度为1410.9个字符。",
    "4": "在最近的742条消息中，18%为other_specific_info类话题（135条），14%为分析图像（104条），11%为文本编辑/润色请求（80条）；其中142条交互质量为高（19%），28条交互质量为低（4%）。",
    "5": "用户目前为ChatGPT Plus订阅用户。",
    "6": "用户目前通过安卓设备的ChatGPT原生App与模型交互。",
    "7": "用户当前的User Agent为：ChatGPT/1.2025.126 (Android 15; 24129PN74C; build 2512605)。",
    "8": "用户在过去1天内活跃1天，过去7天内活跃6天，过去30天内活跃17天。",
    "9": "用户账户注册时间为52周。",
    "10": "用户平均对话深度为3.0。"
  }
}
```

## 5.近期对话内容
```
{
  "Model Set Context": { ... },
  "Assistant Response Preferences": { ... },
  "Notable Past Conversation Topic Highlights": { ... },
  "Helpful User Insights": { ... },
  "User Interaction Metadata": { ... },
  "Recent Conversation Content": {
    "1": "用户于2025年5月25日请求翻译并以 JSON 格式组织其 ChatGPT 用户档案结构，包括模型设定上下文与响应偏好等模块。",
    "2": "用户在5月24日前后连续开展「内容已隐去」工作",
    "3": "在5月21日前后，用户探讨了「内容已隐去」",
    "4": "5月18日至20日之间，用户撰写多篇关于「内容已隐去」",
    "5": "用户在5月17日开展多个技术类话题的问答，包括「内容已隐去」等。",
    "6": "5月中旬，用户持续开发「内容已隐去」",
    "7": "用户在5月初深入探讨了「内容已隐去」",
    "8": "用户使用了图像输入功能，多次请求对枪械、药品包装等照片进行识别和背景分析。",
    "9": "用户自5月以来显著提升互动深度，从设定请求延伸至脚本编写、数据可视化、技术部署流程全链条。"
  }
}
```

# 二、简单的设计机制总结
在前面我们已经大致知道了 ChatGPT 的“记忆”到底是什么东西，它不是真的在记忆，而是一种系统层面的语义增强机制。那么，这套机制到底是怎么运行的？
## 1.“记忆”不是模型本体的能力，而是外挂
首先要明确一点：**大语言模型本身是无记忆的**。

所谓的“记忆”，并不是模型内部参数的更新或状态的延续，而是通过系统层主动注入提示（prompt injection）来**模拟一种“持续了解你”的效果**。从架构上看，整个记忆系统被设计为模型调用流程的外围外挂（wrapper）模块，主要负责两件事：
- **用户画像构建**：后台系统持续从对话中提取信息，形成“你是谁、你关心什么、你喜欢什么”的多维度抽象。
- **上下文注入**：在你每次发起请求时，从这些画像中提取出与当前任务最相关的几条提示信息，合并进提示词中，然后统一交给模型处理。

所以，ChatGPT 能“记得你喜欢咖啡”，不是因为它记住了这个事实，而是因为系统每次都在偷偷提醒它：“这位用户喜欢咖啡”。这也是为什么模型的行为会随着你关闭/开启记忆开关而发生显著变化——你改变的不是模型的记忆，而是**系统是否允许插入这些提示**。
## 2.核心模块一览
OpenAI 当前使用的是一种**分层式的用户状态注入系统**，这套系统目前大致由以下六种模块组成：

| 模块名称                                       | 功能描述                         | 是否用户可见 | 是否可编辑 |
| ------------------------------------------ | ---------------------------- | ------ | ----- |
| Model Set Context                          | 显式保存的个人事实（例如“我是xx工程师”）       | ✅ 是    | ✅ 是   |
| Assistant Response Preferences             | 模型应采用的回应风格、技术细节控制（如偏好结构化输出）  | ❌ 否    | ❌ 否   |
| Notable Past Conversation Topic Highlights | 你经常聊的主题摘要（如“RAG”、“金融市场”）     | ❌ 否    | ❌ 否   |
| Helpful User Insights                      | 系统推导出的事实性描述（如“用户熟练掌握Python”） | ❌ 否    | ❌ 否   |
| Recent Conversation Content                | 最近的聊天摘要（40条以内）               | ❌ 否    | ❌ 否   |
| User Interaction Metadata                  | 账户行为数据（如设备、模型使用比例）           | ❌ 否    | ❌ 否   |
其中，**只有最上面一项 `Model Set Context` 是真正对用户开放管理的**，其余部分全部隐藏在系统层，由模型和后台服务自动维护与调用。用户每次和 ChatGPT 对话时，系统在背后会挑选出若干“你曾说过”、“你可能想要”、“你经常关注”的内容，然后偷偷塞进提示中。这个过程通常会提取 5～20 条信息片段，并在每一轮请求中动态调整，构成一种“临时记忆拼贴”。

## 3.记忆不是简单储存，而是语义压缩与匹配
与我们直觉上的“记忆 = 存信息”不同，ChatGPT 的记忆设计是一种**语义压缩与动态召回机制**，更类似于以下过程：

1. 用户长期对话内容进入嵌入向量空间；
    
2. 系统将这些内容抽象为低维语义向量（embedding）；
    
3. 当前请求被编码为新的向量后，与历史语义向量进行相似度计算；
    
4. 匹配到的“最相关历史内容”被提取、结构化并注入提示词中；
    
5. 最终模型获得一个“仿佛记住你”的输入结构。
    
这比暴力查找更高效，也更灵活，因为它支持语义模糊匹配、上下文重构与用户偏好迁移（即使你换了说法，系统也能识别你在说同一个东西）。

也正因如此，ChatGPT 的“记忆”并不总是准确复述，而是常常表现为“理解你的倾向”。它不是存档式记忆，而是“反应式记忆”。

## 4.记忆并不全依赖你“说了什么”
用户画像的形成**不依赖你明确说了什么、写了什么**，而是系统主动从语义层进行归纳。  
举个例子：

> 你问：“帮我写一份针对区块链创业项目的投资人路演PPT”

系统可能会自动得出：
- 你从事或感兴趣的领域：加密市场 / 创投
- 你具备一定技术与财务建模能力
- 你倾向使用结构化、输出导向型语言
    
这一条“Helpful Insight”就被加入到你的用户画像中，尽管你从没明确告诉过模型这些事。

也就是说，**ChatGPT 不是在等你喂信息，而是在暗中提取你每一句话的“上下文后设含义”**。ChatGPT 的记忆系统其实不是在存你说过什么，而是在逐步构建一个**与你高度重合的人格镜像（persona）**：  它知道你怎么说话、说什么话题、用什么结构、在意哪些细节——然后在你发起下次对话时，加载一个“定制版的你”来与之对应。

这意味着：每个用户看到的 ChatGPT，其实都是**不同的“你版本”所触发的 ChatGPT**。模型始终是同一个模型，但你和我眼中的 ChatGPT，很可能拥有完全不同的“人格面具”。

# 三、实践中我们为什么需要模型记忆，及如何实现记忆

从工程的角度，ChatGPT的记忆机制固然非常精妙，但我们也不应陷入某种误区：**以为只有 OpenAI 那样的“高精度用户画像”才叫记忆**。我们最终需要的应是一个权衡了系统复杂度、运行速度、成本三者之后的，可以工程落地的系统。

在实践中，大模型“记忆”机制的设计目标，可以拆解为三个层次：
## 1.最基本的：上下文维护
这也是目前绝大多数chatbot应用场景中所使用的做法——依靠模型的 context window，把前几轮对话作为输入拼接进去。优点是简单直接、实现成本低，不涉及长期存储或用户画像系统；  
缺点也显而易见：

- 上下文长度有限（尤其在不使用长上下文模型时）
- 无法跨会话使用，关掉网页就“失忆”
- 模型在响应中不会有“我记得你”的个性化表现

这更像是一种**假装有记忆**，而非真正的“记忆机制”，当然，其实这种模式在目前绝大多数chatbot场景中都是够用的。

## 2.中间形态：模糊语义记忆
在这一级，系统开始主动做一些“用户偏好”或“历史行为”的抽象归纳，构建一个**轻量、模糊、非结构化**的长期记忆机制。

典型实现方式包括：

- 将用户对话摘要后存入向量数据库（如FAISS、Weaviate）
- 维护标签系统（如“Python 用户”、“偏好 markdown 输出”）
- 引入 session id 或 user id 作为索引，对当前请求语义进行 recall

这类“记忆”是**可扩展的、弱绑定的、具模糊召回能力的**，尤其适合如AI客服，用户推荐系统，和那种轻量知识增强（如上下文补全型代码助手）等场景。在这个视角下，ai 并不试图准确记住你说过的每句话，而是尽可能提取你的行为特征，在你发起下一个请求时，用模糊而贴近的“印象”来提升模型的回应质量。

## 3.高精度记忆：结构化+动态注入
这也就是OpenAI目前所采用的模式。
- 显式保存的用户设定（如模型设定上下文）
- 隐式提取的行为洞察（如 topic highlights, insights）
- 响应风格控制（Assistant Preferences）
- 自动语义相似度计算与动态提示注入（Embedding-based Memory Routing）

优点是：

- 用户个性化程度高，响应质量贴近用户意图    
- 能长期积累用户资料并自我优化
- 可拓展为插件调用、工作流调度、agent人格等更高阶能力

但它也有代价：

- 用户看不到的那部分“记忆”变成了不可控的灰盒结构
- 需要配套一整套后台服务来做状态管理、embedding存储与匹配
- 极大提高了系统工程复杂度与维护成本

这类设计适合的，是构建**长期陪伴型智能体、个人AI助手**、或大规模用户多轮交互平台。未来各家的主流ai助手软件大概率都会走这条路。

我自己也实现过一套“猴版”的长期记忆系统。整体思路是：每当对话进行到第十五轮，就调用一次 Gemini Flash，从当前对话中抽取潜在需要记住的信息，并为每条内容分配一个优先级权重，存入一个 JSON 文件中。

这些记忆条目会根据优先级逐渐被“遗忘”或更新。每次新对话开始时，系统会从中选取 5 条——**最近更新的、优先级最高的、并加入一定随机性**——插入到上下文中，从而模拟出一种简易的长期记忆效果。这个方案不依赖任何 embedding 模型或向量数据库，纯靠 prompt 操作和权重管理维持运行。虽然很猴版，但实际使用体验也还算凑合，能在不少轻量场景下跑得起来。

>真的，我觉得大道至简，大家扪心自问一下其实绝大多数chatbot场景用哥们上面那套用法是完全可行的，效果也未必会输给那些无脑塞向量数据库的记忆方案，整体系统复杂度也低好修改